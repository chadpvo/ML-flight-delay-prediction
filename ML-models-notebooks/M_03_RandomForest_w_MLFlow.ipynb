{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00eb1c09-9a74-4d23-a18b-28507c203e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94721703-eb06-40a9-bc57-4c8bb8066a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af098438-a1eb-48c8-8aa1-03e7f13ba24e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Standard Imports"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer, Imputer\n",
    "from pyspark.sql.functions import log1p, unix_timestamp, to_date, col, when, lit, row_number, hour, dayofweek, minute, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from functools import reduce\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import math\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from itertools import product\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) #ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29a05d26-d893-42a3-821e-2a16d41133a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Setup Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dbcd62f-fbe7-4b2d-8029-8d47d3f14389",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Time Split Selector"
    }
   },
   "outputs": [],
   "source": [
    "def data_set(time):\n",
    "    if time == 3:\n",
    "        return \"_3m\"\n",
    "    elif time == 6:\n",
    "        return \"_6m\"\n",
    "    elif time == 12:\n",
    "        return \"_1y\"\n",
    "    elif time == 'all':\n",
    "        return \"\"\n",
    "    else:\n",
    "        raise ValueError(\"time must be 3, 6, 12, or 'all'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7918571-4cf0-4c83-bf19-7010d26c6f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Paths for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354239d1-77d7-4101-9e1b-ac5be41df39e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Training Path"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folds from: dbfs:/student-groups/Group_02_01/fasw/processed_rolling_windows\nDetected 2 rolling windows\n"
     ]
    }
   ],
   "source": [
    "dbfs_path = \"dbfs:/student-groups/Group_02_01\"\n",
    "splits_path = f\"{dbfs_path}/fasw\"\n",
    "folds_input_path = f\"{splits_path}/processed_rolling_windows\"\n",
    "\n",
    "window_dirs = dbutils.fs.ls(folds_input_path)\n",
    "train_windows = [d.name for d in window_dirs if d.name.startswith(\"window_\") and d.name.endswith(\"_train/\")]\n",
    "N_SPLITS = len(train_windows)\n",
    "\n",
    "print(f\"Loading folds from: {folds_input_path}\")\n",
    "print(f\"Detected {N_SPLITS} rolling windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90704c5b-5130-41f8-bafb-bbd3c2328605",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quick Path Check & Null Check"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column                                             Type                     Null %\n----------------------------------------------------------------------------------\nYEAR                                               IntegerType()             0.00%\nQUARTER                                            IntegerType()             0.00%\nMONTH                                              IntegerType()             0.00%\nDAY_OF_MONTH                                       IntegerType()             0.00%\nDAY_OF_WEEK                                        IntegerType()             0.00%\nIS_US_HOLIDAY                                      DoubleType()              0.00%\nOP_UNIQUE_CARRIER                                  StringType()              0.00%\nTAIL_NUM                                           StringType()              0.00%\nORIGIN                                             StringType()              0.00%\nORIGIN_STATE_ABR                                   StringType()              0.00%\nDEST                                               StringType()              0.00%\nDEST_STATE_ABR                                     StringType()              0.00%\nCRS_DEP_TIME_BLOCK                                 StringType()              0.00%\nCRS_DEP_DATETIME_UTC                               TimestampType()           0.00%\nCRS_ARR_TIME_BLOCK                                 StringType()              0.00%\nARR_DEL15                                          IntegerType()             0.00%\nCRS_ELAPSED_TIME                                   DoubleType()              0.00%\nCARRIER_DELAY                                      IntegerType()             0.00%\nNAS_DELAY                                          IntegerType()             0.00%\nSECURITY_DELAY                                     IntegerType()             0.00%\nLATE_AIRCRAFT_DELAY                                IntegerType()             0.00%\nDISTANCE                                           DoubleType()              0.00%\nORIGIN_LAT                                         DoubleType()              0.00%\nORIGIN_LONG                                        DoubleType()              0.00%\nORIGIN_ELEVATION_FT                                IntegerType()             0.00%\nORIGIN_SIZE                                        StringType()              0.00%\nDEST_LAT                                           DoubleType()              0.00%\nDEST_LON                                           DoubleType()              0.00%\nDEST_ELEVATION_FT                                  IntegerType()             0.00%\nDEST_SIZE                                          StringType()              0.00%\noverall_cloud_frac_0_1                             DoubleType()              0.00%\nlowest_cloud_ft                                    IntegerType()             0.00%\nhighest_cloud_ft                                   IntegerType()             0.00%\nhas_few                                            DoubleType()              0.00%\nhas_sct                                            DoubleType()              0.00%\nhas_bkn                                            DoubleType()              0.00%\nhas_ovc                                            DoubleType()              0.00%\nHourlyAltimeterSetting                             DoubleType()              0.00%\nHourlyWindCardinalDirection                        StringType()              0.00%\nHourlyWindGustSpeed                                IntegerType()             0.00%\nHourlyWindSpeed                                    IntegerType()             0.00%\nlight                                              DoubleType()              0.00%\nheavy                                              DoubleType()              0.00%\nthunderstorm                                       DoubleType()              0.00%\nrain_or_drizzle                                    DoubleType()              0.00%\nfreezing_conditions                                DoubleType()              0.00%\nsnow                                               DoubleType()              0.00%\nhail_or_ice                                        DoubleType()              0.00%\nreduced_visibility                                 DoubleType()              0.00%\nspatial_effects                                    DoubleType()              0.00%\nunknown_precip                                     DoubleType()              0.00%\norigin_pagerank                                    DoubleType()              0.00%\ndest_pagerank                                      DoubleType()              0.00%\norigin_out_degree                                  DoubleType()              0.00%\ndest_in_degree                                     DoubleType()              0.00%\nprev_flight_arr_delay_clean                        DoubleType()              0.00%\nactual_to_crs_time_to_next_flight_diff_mins_clean  DoubleType()              0.00%\ncrs_time_to_next_flight_diff_mins                  DoubleType()              0.00%\nCRS_ELAPSED_TIME_log                               DoubleType()              0.00%\nDISTANCE_log                                       DoubleType()              0.00%\nORIGIN_ELEVATION_FT_log                            DoubleType()              0.00%\nDEST_ELEVATION_FT_log                              DoubleType()              0.00%\nlowest_cloud_ft_log                                DoubleType()              0.00%\nHourlyWindGustSpeed_log                            DoubleType()              0.00%\nHourlyWindSpeed_log                                DoubleType()              0.00%\ncrs_time_to_next_flight_diff_mins_log              DoubleType()              0.00%\nactual_to_crs_time_to_next_flight_diff_mins_clean_log DoubleType()              0.00%\nOP_UNIQUE_CARRIER_idx                              DoubleType()              0.00%\nOP_UNIQUE_CARRIER_ohe                              VectorUDT()               0.00%\nORIGIN_idx                                         DoubleType()              0.00%\nORIGIN_ohe                                         VectorUDT()               0.00%\nORIGIN_STATE_ABR_idx                               DoubleType()              0.00%\nORIGIN_STATE_ABR_ohe                               VectorUDT()               0.00%\nDEST_idx                                           DoubleType()              0.00%\nDEST_ohe                                           VectorUDT()               0.00%\nDEST_STATE_ABR_idx                                 DoubleType()              0.00%\nDEST_STATE_ABR_ohe                                 VectorUDT()               0.00%\nORIGIN_SIZE_idx                                    DoubleType()              0.00%\nORIGIN_SIZE_ohe                                    VectorUDT()               0.00%\nDEST_SIZE_idx                                      DoubleType()              0.00%\nDEST_SIZE_ohe                                      VectorUDT()               0.00%\nHourlyWindCardinalDirection_idx                    DoubleType()              0.00%\nHourlyWindCardinalDirection_ohe                    VectorUDT()               0.00%\nQUARTER_idx                                        DoubleType()              0.00%\nQUARTER_ohe                                        VectorUDT()               0.00%\nMONTH_idx                                          DoubleType()              0.00%\nMONTH_ohe                                          VectorUDT()               0.00%\nDAY_OF_MONTH_idx                                   DoubleType()              0.00%\nDAY_OF_MONTH_ohe                                   VectorUDT()               0.00%\nDAY_OF_WEEK_idx                                    DoubleType()              0.00%\nDAY_OF_WEEK_ohe                                    VectorUDT()               0.00%\nCRS_DEP_TIME_BLOCK_idx                             DoubleType()              0.00%\nCRS_DEP_TIME_BLOCK_ohe                             VectorUDT()               0.00%\nCRS_ARR_TIME_BLOCK_idx                             DoubleType()              0.00%\nCRS_ARR_TIME_BLOCK_ohe                             VectorUDT()               0.00%\nnum_vector                                         VectorUDT()               0.00%\nscaled_num_vector                                  VectorUDT()               0.00%\nohe_vector                                         VectorUDT()               0.00%\nlog_scaled_features                                VectorUDT()               0.00%\nlog_unscaled_features                              VectorUDT()               0.00%\nfull_unscaled_features                             VectorUDT()               0.00%\n"
     ]
    }
   ],
   "source": [
    "sample_window_path = f\"{folds_input_path}/window_2_train\"\n",
    "df_sample = spark.read.parquet(sample_window_path)\n",
    "\n",
    "null_counts = (\n",
    "    df_sample.select([\n",
    "        F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "        for c in df_sample.columns\n",
    "    ])\n",
    "    .collect()[0]\n",
    "    .asDict()\n",
    ")\n",
    "\n",
    "total_rows = df_sample.count()\n",
    "\n",
    "print(f\"{'Column':<50} {'Type':<20} {'Null %':>10}\")\n",
    "print(\"-\" * 82)\n",
    "\n",
    "for field in df_sample.schema.fields:\n",
    "    col_name = field.name\n",
    "    dtype = str(field.dataType)\n",
    "    null_count = null_counts.get(col_name, 0)\n",
    "    null_pct = (null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "    print(f\"{col_name:<50} {dtype:<20} {null_pct:>9.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47790e36-65ce-425e-abc4-6ac961c93e7a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Windows Columns Alignment Check"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_1_train/: 101 columns\n  Full column list: ['YEAR', 'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'IS_US_HOLIDAY', 'OP_UNIQUE_CARRIER', 'TAIL_NUM', 'ORIGIN', 'ORIGIN_STATE_ABR']... (showing first 10)\n  Class distribution -> 1: 2027479 (49.98%), 0: 2029369 (50.02%)\n\nwindow_2_train/: 101 columns\n  Full column list: ['YEAR', 'QUARTER', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'IS_US_HOLIDAY', 'OP_UNIQUE_CARRIER', 'TAIL_NUM', 'ORIGIN', 'ORIGIN_STATE_ABR']... (showing first 10)\n  Class distribution -> 1: 1993288 (49.98%), 0: 1995228 (50.02%)\n\n\n================================================================================\nColumns present in ALL windows (101):\n['ARR_DEL15', 'CARRIER_DELAY', 'CRS_ARR_TIME_BLOCK', 'CRS_ARR_TIME_BLOCK_idx', 'CRS_ARR_TIME_BLOCK_ohe', 'CRS_DEP_DATETIME_UTC', 'CRS_DEP_TIME_BLOCK', 'CRS_DEP_TIME_BLOCK_idx', 'CRS_DEP_TIME_BLOCK_ohe', 'CRS_ELAPSED_TIME', 'CRS_ELAPSED_TIME_log', 'DAY_OF_MONTH', 'DAY_OF_MONTH_idx', 'DAY_OF_MONTH_ohe', 'DAY_OF_WEEK', 'DAY_OF_WEEK_idx', 'DAY_OF_WEEK_ohe', 'DEST', 'DEST_ELEVATION_FT', 'DEST_ELEVATION_FT_log', 'DEST_LAT', 'DEST_LON', 'DEST_SIZE', 'DEST_SIZE_idx', 'DEST_SIZE_ohe', 'DEST_STATE_ABR', 'DEST_STATE_ABR_idx', 'DEST_STATE_ABR_ohe', 'DEST_idx', 'DEST_ohe', 'DISTANCE', 'DISTANCE_log', 'HourlyAltimeterSetting', 'HourlyWindCardinalDirection', 'HourlyWindCardinalDirection_idx', 'HourlyWindCardinalDirection_ohe', 'HourlyWindGustSpeed', 'HourlyWindGustSpeed_log', 'HourlyWindSpeed', 'HourlyWindSpeed_log', 'IS_US_HOLIDAY', 'LATE_AIRCRAFT_DELAY', 'MONTH', 'MONTH_idx', 'MONTH_ohe', 'NAS_DELAY', 'OP_UNIQUE_CARRIER', 'OP_UNIQUE_CARRIER_idx', 'OP_UNIQUE_CARRIER_ohe', 'ORIGIN', 'ORIGIN_ELEVATION_FT', 'ORIGIN_ELEVATION_FT_log', 'ORIGIN_LAT', 'ORIGIN_LONG', 'ORIGIN_SIZE', 'ORIGIN_SIZE_idx', 'ORIGIN_SIZE_ohe', 'ORIGIN_STATE_ABR', 'ORIGIN_STATE_ABR_idx', 'ORIGIN_STATE_ABR_ohe', 'ORIGIN_idx', 'ORIGIN_ohe', 'QUARTER', 'QUARTER_idx', 'QUARTER_ohe', 'SECURITY_DELAY', 'TAIL_NUM', 'YEAR', 'actual_to_crs_time_to_next_flight_diff_mins_clean', 'actual_to_crs_time_to_next_flight_diff_mins_clean_log', 'crs_time_to_next_flight_diff_mins', 'crs_time_to_next_flight_diff_mins_log', 'dest_in_degree', 'dest_pagerank', 'freezing_conditions', 'full_unscaled_features', 'hail_or_ice', 'has_bkn', 'has_few', 'has_ovc', 'has_sct', 'heavy', 'highest_cloud_ft', 'light', 'log_scaled_features', 'log_unscaled_features', 'lowest_cloud_ft', 'lowest_cloud_ft_log', 'num_vector', 'ohe_vector', 'origin_out_degree', 'origin_pagerank', 'overall_cloud_frac_0_1', 'prev_flight_arr_delay_clean', 'rain_or_drizzle', 'reduced_visibility', 'scaled_num_vector', 'snow', 'spatial_effects', 'thunderstorm', 'unknown_precip']\n\n================================================================================\nColumns found in SOME but not all windows (0):\n\n================================================================================\nPer-window missing columns:\n\nwindow_1_train/ has all columns.\n--------------------------------------------------\nwindow_2_train/ has all columns.\n--------------------------------------------------\nTotal unique columns across all windows: 101\nCommon columns in ALL windows: 101\nInconsistent columns: 0\n"
     ]
    }
   ],
   "source": [
    "window_col_dict = {}\n",
    "window_col_list = {}\n",
    "\n",
    "for w in train_windows:\n",
    "    path = f\"{folds_input_path}/{w}\"\n",
    "    df = spark.read.parquet(path)\n",
    "    \n",
    "    # --- Column info ---\n",
    "    cols = set(df.columns)\n",
    "    col_list = df.columns\n",
    "    window_col_dict[w] = cols\n",
    "    window_col_list[w] = col_list\n",
    "    print(f\"{w}: {len(cols)} columns\")\n",
    "    print(f\"  Full column list: {col_list[:10]}... (showing first 10)\")\n",
    "    \n",
    "    # --- Class imbalance check ---\n",
    "    label_counts = df.groupBy(\"ARR_DEL15\").count().collect()\n",
    "    counts_dict = {row['ARR_DEL15']: row['count'] for row in label_counts}\n",
    "    num_pos = counts_dict.get(1, 0)\n",
    "    num_neg = counts_dict.get(0, 0)\n",
    "    total = num_pos + num_neg\n",
    "    pos_pct = num_pos / total * 100 if total > 0 else 0\n",
    "    neg_pct = num_neg / total * 100 if total > 0 else 0\n",
    "    print(f\"  Class distribution -> 1: {num_pos} ({pos_pct:.2f}%), 0: {num_neg} ({neg_pct:.2f}%)\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Columns that appear in ALL windows\n",
    "common_cols = set.intersection(*window_col_dict.values())\n",
    "print(f\"Columns present in ALL windows ({len(common_cols)}):\")\n",
    "print(sorted(list(common_cols)))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Columns that differ between windows\n",
    "all_cols = set.union(*window_col_dict.values())\n",
    "print(f\"Columns found in SOME but not all windows ({len(all_cols - common_cols)}):\")\n",
    "for col in sorted(list(all_cols - common_cols)):\n",
    "    windows_with_col = [w for w, cols in window_col_dict.items() if col in cols]\n",
    "    print(f\"  {col}: present in {len(windows_with_col)}/{len(train_windows)} windows - {windows_with_col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Detailed per-window missing columns\n",
    "print(\"Per-window missing columns:\\n\")\n",
    "for w, cols in window_col_dict.items():\n",
    "    missing = all_cols - cols\n",
    "    if missing:\n",
    "        print(f\"{w} is MISSING {len(missing)} columns:\")\n",
    "        print(sorted(list(missing)))\n",
    "    else:\n",
    "        print(f\"{w} has all columns.\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "print(f\"Total unique columns across all windows: {len(all_cols)}\")\n",
    "print(f\"Common columns in ALL windows: {len(common_cols)}\")\n",
    "print(f\"Inconsistent columns: {len(all_cols - common_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608072f6-fb6e-4baa-9cbf-f3bf4c4484e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Path for Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce0ff0c-581c-4dc0-8ecf-b73ffc5f5f05",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuring Test Dataset Path Based on Time Split"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blind Test set path: dbfs:/student-groups/Group_02_01/fasw/processed_train_test/test\nBlind Train set path: dbfs:/student-groups/Group_02_01/fasw/processed_train_test/train\n"
     ]
    }
   ],
   "source": [
    "blind_test_set_path = f\"dbfs:/student-groups/Group_02_01/fasw/processed_train_test/test\"\n",
    "blind_train_set_path = f\"dbfs:/student-groups/Group_02_01/fasw/processed_train_test/train\"\n",
    "\n",
    "print(f\"Blind Test set path: {blind_test_set_path}\")\n",
    "print(f\"Blind Train set path: {blind_train_set_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30710c7a-0a42-4db1-b6d6-0bb284accf82",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Class Imbalance Check in Blind Test/Train Data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: dbfs:/student-groups/Group_02_01/fasw/processed_train_test/train\n  Total rows: 23,873,225\n  ARR_DEL15=1: 4,405,402 (18.45%)\n  ARR_DEL15=0: 19,467,823 (81.55%)\n------------------------------------------------------------\nPath: dbfs:/student-groups/Group_02_01/fasw/processed_train_test/test\n  Total rows: 7,271,711\n  ARR_DEL15=1: 1,387,904 (19.09%)\n  ARR_DEL15=0: 5,883,807 (80.91%)\n------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def class_imbalance_report(path, label_col=\"ARR_DEL15\"):\n",
    "    df = spark.read.parquet(path)\n",
    "    total = df.count()\n",
    "    counts = df.groupBy(label_col).count().toPandas().set_index(label_col)['count'].to_dict()\n",
    "    num_pos = counts.get(1, 0)\n",
    "    num_neg = counts.get(0, 0)\n",
    "    pos_pct = num_pos / total * 100 if total > 0 else 0\n",
    "    neg_pct = num_neg / total * 100 if total > 0 else 0\n",
    "    print(f\"Path: {path}\")\n",
    "    print(f\"  Total rows: {total:,}\")\n",
    "    print(f\"  {label_col}=1: {num_pos:,} ({pos_pct:.2f}%)\")\n",
    "    print(f\"  {label_col}=0: {num_neg:,} ({neg_pct:.2f}%)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "class_imbalance_report(blind_train_set_path)\n",
    "class_imbalance_report(blind_test_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "356772dc-f5c8-4949-a350-d55f38b52ffd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Downsampling Code"
    }
   },
   "outputs": [],
   "source": [
    "def downsample(train_df, target_ratio=1.0, verbose=False):\n",
    "    '''Downsamples train_df to balance classes'''\n",
    "    delay_count = train_df.filter(F.col(\"ARR_DEL15\") == 1).count()\n",
    "    non_delay_count = train_df.filter(F.col(\"ARR_DEL15\") == 0).count()\n",
    "    \n",
    "    keep_percent = (delay_count / non_delay_count) / target_ratio\n",
    "\n",
    "    if keep_percent >= 1.0:\n",
    "        print(\"Warning: Target ratio is already balanced or majority is smaller. No sampling applied.\")\n",
    "        return train_df\n",
    "    \n",
    "    train_delay = train_df.filter(F.col('ARR_DEL15') == 1)\n",
    "    train_non_delay = train_df.filter(F.col('ARR_DEL15') == 0).sample(withReplacement=False, fraction=keep_percent, seed=42)\n",
    "    train_downsampled = train_delay.union(train_non_delay)\n",
    "    return train_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76d34f04-c4da-47e5-95db-5586af6a4fda",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Up Master Training Path for Blind Test"
    }
   },
   "outputs": [],
   "source": [
    "df_train_master = spark.read.parquet(blind_train_set_path)\n",
    "df_train_master = downsample(df_train_master, target_ratio=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a116f896-e073-4af1-a1a1-bc42de4951be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Master Train Class Imbalance Check"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ARR_DEL15</th><th>count</th></tr></thead><tbody><tr><td>0</td><td>4405998</td></tr><tr><td>1</td><td>4405402</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0,
         4405998
        ],
        [
         1,
         4405402
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ARR_DEL15",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_master_class_balance_check = df_train_master.groupBy(\"ARR_DEL15\").count().orderBy(\"ARR_DEL15\")\n",
    "display(train_master_class_balance_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d730216-900f-435a-814e-ac69ad09c348",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initial Null Check for Raw Test Path"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Analysis: dbfs:/student-groups/Group_02_01/fasw/processed_train_test/test\nTotal rows: 7,271,711\nTotal columns: 101\n\nColumn                                             Type                     Null %\n----------------------------------------------------------------------------------\nYEAR                                               IntegerType()             0.00%\nQUARTER                                            IntegerType()             0.00%\nMONTH                                              IntegerType()             0.00%\nDAY_OF_MONTH                                       IntegerType()             0.00%\nDAY_OF_WEEK                                        IntegerType()             0.00%\nIS_US_HOLIDAY                                      DoubleType()              0.00%\nOP_UNIQUE_CARRIER                                  StringType()              0.00%\nTAIL_NUM                                           StringType()              0.00%\nORIGIN                                             StringType()              0.00%\nORIGIN_STATE_ABR                                   StringType()              0.00%\nDEST                                               StringType()              0.00%\nDEST_STATE_ABR                                     StringType()              0.00%\nCRS_DEP_TIME_BLOCK                                 StringType()              0.00%\nCRS_DEP_DATETIME_UTC                               TimestampType()           0.00%\nCRS_ARR_TIME_BLOCK                                 StringType()              0.00%\nARR_DEL15                                          IntegerType()             0.00%\nCRS_ELAPSED_TIME                                   DoubleType()              0.00%\nCARRIER_DELAY                                      IntegerType()             0.00%\nNAS_DELAY                                          IntegerType()             0.00%\nSECURITY_DELAY                                     IntegerType()             0.00%\nLATE_AIRCRAFT_DELAY                                IntegerType()             0.00%\nDISTANCE                                           DoubleType()              0.00%\nORIGIN_LAT                                         DoubleType()              0.00%\nORIGIN_LONG                                        DoubleType()              0.00%\nORIGIN_ELEVATION_FT                                IntegerType()             0.00%\nORIGIN_SIZE                                        StringType()              0.00%\nDEST_LAT                                           DoubleType()              0.00%\nDEST_LON                                           DoubleType()              0.00%\nDEST_ELEVATION_FT                                  IntegerType()             0.00%\nDEST_SIZE                                          StringType()              0.00%\noverall_cloud_frac_0_1                             DoubleType()              0.00%\nlowest_cloud_ft                                    IntegerType()             0.00%\nhighest_cloud_ft                                   IntegerType()             0.00%\nhas_few                                            DoubleType()              0.00%\nhas_sct                                            DoubleType()              0.00%\nhas_bkn                                            DoubleType()              0.00%\nhas_ovc                                            DoubleType()              0.00%\nHourlyAltimeterSetting                             DoubleType()              0.00%\nHourlyWindCardinalDirection                        StringType()              0.00%\nHourlyWindGustSpeed                                IntegerType()             0.00%\nHourlyWindSpeed                                    IntegerType()             0.00%\nlight                                              DoubleType()              0.00%\nheavy                                              DoubleType()              0.00%\nthunderstorm                                       DoubleType()              0.00%\nrain_or_drizzle                                    DoubleType()              0.00%\nfreezing_conditions                                DoubleType()              0.00%\nsnow                                               DoubleType()              0.00%\nhail_or_ice                                        DoubleType()              0.00%\nreduced_visibility                                 DoubleType()              0.00%\nspatial_effects                                    DoubleType()              0.00%\nunknown_precip                                     DoubleType()              0.00%\norigin_pagerank                                    DoubleType()              0.00%\ndest_pagerank                                      DoubleType()              0.00%\norigin_out_degree                                  DoubleType()              0.00%\ndest_in_degree                                     DoubleType()              0.00%\nprev_flight_arr_delay_clean                        DoubleType()              0.00%\nactual_to_crs_time_to_next_flight_diff_mins_clean  DoubleType()              0.00%\ncrs_time_to_next_flight_diff_mins                  DoubleType()              0.00%\nCRS_ELAPSED_TIME_log                               DoubleType()              0.00%\nDISTANCE_log                                       DoubleType()              0.00%\nORIGIN_ELEVATION_FT_log                            DoubleType()              0.00%\nDEST_ELEVATION_FT_log                              DoubleType()              0.00%\nlowest_cloud_ft_log                                DoubleType()              0.00%\nHourlyWindGustSpeed_log                            DoubleType()              0.00%\nHourlyWindSpeed_log                                DoubleType()              0.00%\ncrs_time_to_next_flight_diff_mins_log              DoubleType()              0.00%\nactual_to_crs_time_to_next_flight_diff_mins_clean_log DoubleType()              0.00%\nOP_UNIQUE_CARRIER_idx                              DoubleType()              0.00%\nOP_UNIQUE_CARRIER_ohe                              VectorUDT()               0.00%\nORIGIN_idx                                         DoubleType()              0.00%\nORIGIN_ohe                                         VectorUDT()               0.00%\nORIGIN_STATE_ABR_idx                               DoubleType()              0.00%\nORIGIN_STATE_ABR_ohe                               VectorUDT()               0.00%\nDEST_idx                                           DoubleType()              0.00%\nDEST_ohe                                           VectorUDT()               0.00%\nDEST_STATE_ABR_idx                                 DoubleType()              0.00%\nDEST_STATE_ABR_ohe                                 VectorUDT()               0.00%\nORIGIN_SIZE_idx                                    DoubleType()              0.00%\nORIGIN_SIZE_ohe                                    VectorUDT()               0.00%\nDEST_SIZE_idx                                      DoubleType()              0.00%\nDEST_SIZE_ohe                                      VectorUDT()               0.00%\nHourlyWindCardinalDirection_idx                    DoubleType()              0.00%\nHourlyWindCardinalDirection_ohe                    VectorUDT()               0.00%\nQUARTER_idx                                        DoubleType()              0.00%\nQUARTER_ohe                                        VectorUDT()               0.00%\nMONTH_idx                                          DoubleType()              0.00%\nMONTH_ohe                                          VectorUDT()               0.00%\nDAY_OF_MONTH_idx                                   DoubleType()              0.00%\nDAY_OF_MONTH_ohe                                   VectorUDT()               0.00%\nDAY_OF_WEEK_idx                                    DoubleType()              0.00%\nDAY_OF_WEEK_ohe                                    VectorUDT()               0.00%\nCRS_DEP_TIME_BLOCK_idx                             DoubleType()              0.00%\nCRS_DEP_TIME_BLOCK_ohe                             VectorUDT()               0.00%\nCRS_ARR_TIME_BLOCK_idx                             DoubleType()              0.00%\nCRS_ARR_TIME_BLOCK_ohe                             VectorUDT()               0.00%\nnum_vector                                         VectorUDT()               0.00%\nscaled_num_vector                                  VectorUDT()               0.00%\nohe_vector                                         VectorUDT()               0.00%\nlog_scaled_features                                VectorUDT()               0.00%\nlog_unscaled_features                              VectorUDT()               0.00%\nfull_unscaled_features                             VectorUDT()               0.00%\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.read.parquet(blind_test_set_path)\n",
    "null_counts = (\n",
    "    df_test.select([\n",
    "        F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "        for c in df_test.columns\n",
    "    ])\n",
    "    .collect()[0]\n",
    "    .asDict()\n",
    ")\n",
    "\n",
    "total_rows = df_test.count()\n",
    "\n",
    "print(f\"Test Set Analysis: {blind_test_set_path}\")\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "print(f\"Total columns: {len(df_test.columns)}\")\n",
    "print()\n",
    "print(f\"{'Column':<50} {'Type':<20} {'Null %':>10}\")\n",
    "print(\"-\" * 82)\n",
    "\n",
    "for field in df_test.schema.fields:\n",
    "    col_name = field.name\n",
    "    dtype = str(field.dataType)\n",
    "    null_count = null_counts.get(col_name, 0)\n",
    "    null_pct = (null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "    print(f\"{col_name:<50} {dtype:<20} {null_pct:>9.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "345ceec5-108a-4283-9920-992015f9962a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Test & Train Data Features Comparison"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in TEST data                                      101\nColumns common to ALL training windows                    101\nColumns in ANY training window                            101\nAlighment Check Test & Training Data\nColumns in BOTH test and ALL train windows                101\nColumns ONLY in test (missing from ALL train)               0\nColumns ONLY in ALL train (missing from test)               0\n"
     ]
    }
   ],
   "source": [
    "test_cols = set(df_test.columns)\n",
    "print(f\"{'Columns in TEST data':<50} {len(test_cols):>10}\")\n",
    "print(f\"{'Columns common to ALL training windows':<50} {len(common_cols):>10}\")\n",
    "print(f\"{'Columns in ANY training window':<50} {len(all_cols):>10}\")\n",
    "\n",
    "# Compare test vs common training columns\n",
    "common_test_train = test_cols & common_cols\n",
    "only_in_test = test_cols - common_cols\n",
    "only_in_train = common_cols - test_cols\n",
    "\n",
    "print(f\"Alighment Check Test & Training Data\")\n",
    "print(f\"{'Columns in BOTH test and ALL train windows':<50} {len(common_test_train):>10}\")\n",
    "print(f\"{'Columns ONLY in test (missing from ALL train)':<50} {len(only_in_test):>10}\")\n",
    "print(f\"{'Columns ONLY in ALL train (missing from test)':<50} {len(only_in_train):>10}\")\n",
    "\n",
    "if only_in_train:\n",
    "    print(f\"\\nCOLUMNS IN ALL TRAINING WINDOWS BUT MISSING FROM TEST ({len(only_in_train)}):\")\n",
    "    for col in sorted(only_in_train):\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "if only_in_test:\n",
    "    print(f\"\\nCOLUMNS IN TEST BUT MISSING FROM ALL TRAINING WINDOWS ({len(only_in_test)}):\")\n",
    "    for col in sorted(only_in_test):\n",
    "        print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4165a4f-2f5f-413a-9fac-2a86d4a4ffb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Summary of Nulls/Naan Values on all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc303fbe-a785-4d97-8563-d4104bf1794b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Redefining Features to Use - First Check"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nREMAINING COLUMNS:\n  - QUARTER\n  - MONTH\n  - DAY_OF_MONTH\n  - DAY_OF_WEEK\n  - IS_US_HOLIDAY\n  - ARR_DEL15\n  - CRS_ELAPSED_TIME\n  - DISTANCE\n  - ORIGIN_LAT\n  - ORIGIN_LONG\n  - ORIGIN_ELEVATION_FT\n  - DEST_LAT\n  - DEST_LON\n  - DEST_ELEVATION_FT\n  - overall_cloud_frac_0_1\n  - lowest_cloud_ft\n  - highest_cloud_ft\n  - has_few\n  - has_sct\n  - has_bkn\n  - has_ovc\n  - HourlyAltimeterSetting\n  - HourlyWindGustSpeed\n  - HourlyWindSpeed\n  - light\n  - heavy\n  - thunderstorm\n  - rain_or_drizzle\n  - freezing_conditions\n  - snow\n  - hail_or_ice\n  - reduced_visibility\n  - spatial_effects\n  - unknown_precip\n  - origin_pagerank\n  - dest_pagerank\n  - origin_out_degree\n  - dest_in_degree\n  - prev_flight_arr_delay_clean\n  - actual_to_crs_time_to_next_flight_diff_mins_clean\n  - crs_time_to_next_flight_diff_mins\n  - OP_UNIQUE_CARRIER_idx\n  - ORIGIN_idx\n  - ORIGIN_STATE_ABR_idx\n  - DEST_idx\n  - DEST_STATE_ABR_idx\n  - ORIGIN_SIZE_idx\n  - DEST_SIZE_idx\n  - HourlyWindCardinalDirection_idx\n  - QUARTER_idx\n  - MONTH_idx\n  - DAY_OF_MONTH_idx\n  - DAY_OF_WEEK_idx\n  - CRS_DEP_TIME_BLOCK_idx\n  - CRS_ARR_TIME_BLOCK_idx\n"
     ]
    }
   ],
   "source": [
    "leakage_cols = ['CARRIER_DELAY', 'LATE_AIRCRAFT_DELAY', 'NAS_DELAY', 'SECURITY_DELAY']\n",
    "vector_cols_to_remove = ['num_vector', 'ohe_vector', 'log_scaled_features', 'log_unscaled_features', 'scaled_num_vector', 'full_unscaled_features']\n",
    "ohe_cols = [c for c in df_test.columns if c.endswith('_ohe')]\n",
    "log_cols = [c for c in df_test.columns if c.endswith('_log')]\n",
    "metadata_cols = ['TAIL_NUM', 'CRS_DEP_DATETIME_UTC']\n",
    "cat_string_cols = [\n",
    "    'OP_UNIQUE_CARRIER', 'ORIGIN', 'DEST', 'ORIGIN_STATE_ABR', 'DEST_STATE_ABR',\n",
    "    'ORIGIN_SIZE', 'DEST_SIZE', 'HourlyWindCardinalDirection',\n",
    "    'CRS_DEP_TIME_BLOCK', 'CRS_ARR_TIME_BLOCK'\n",
    "]\n",
    "temporal_numeric_cols = ['YEAR']\n",
    "cols_to_remove = leakage_cols + vector_cols_to_remove + ohe_cols + log_cols + metadata_cols + cat_string_cols + temporal_numeric_cols\n",
    "cols_to_remove = [c for c in cols_to_remove if c in df_test.columns]\n",
    "df_test_final = df_test.drop(*cols_to_remove)\n",
    "feature_cols = [c for c in df_test_final.columns if c != 'ARR_DEL15']\n",
    "\n",
    "print(\"\\nREMAINING COLUMNS:\")\n",
    "for c in df_test_final.columns:\n",
    "    print(\"  -\", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb3ab78d-dfdb-4ce1-8e05-dc606cc25b41",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Features Selection"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE DEFINITIONS FOR MODEL\nNumeric columns: 26\nIndexed categorical columns: 9\nTotal model features: 35\n\nPresent in test: 35/35\nMissing from test: 0\n"
     ]
    }
   ],
   "source": [
    "available_cols = set(df_test_final.columns)\n",
    "\n",
    "numeric_cols_to_use = [\n",
    "    'DISTANCE', \n",
    "    'ORIGIN_ELEVATION_FT', \n",
    "    'DEST_ELEVATION_FT', \n",
    "    'overall_cloud_frac_0_1', \n",
    "    'lowest_cloud_ft', \n",
    "    'highest_cloud_ft', \n",
    "    # 'has_few',                  # 0.0007% - REMOVED\n",
    "    # 'has_sct',                  # 0.0006% - REMOVED\n",
    "    # 'has_bkn',                  # 0.0008% - REMOVED\n",
    "    'has_ovc', \n",
    "    'HourlyAltimeterSetting', \n",
    "    'HourlyWindGustSpeed', \n",
    "    'HourlyWindSpeed', \n",
    "    'light', \n",
    "    # 'heavy',                    # 0.0001% - REMOVED\n",
    "    # 'thunderstorm',             # 0.0003% - REMOVED\n",
    "    'rain_or_drizzle', \n",
    "    # 'freezing_conditions',      # 0.0005% - REMOVED\n",
    "    'snow', \n",
    "    # 'hail_or_ice',              # 0.0000% - REMOVED\n",
    "    'reduced_visibility', \n",
    "    # 'spatial_effects',          # 0.0000% - REMOVED\n",
    "    # 'unknown_precip',           # 0.0000% - REMOVED\n",
    "    'ORIGIN_LAT', \n",
    "    'ORIGIN_LONG', \n",
    "    'DEST_LAT', \n",
    "    'DEST_LON', \n",
    "    'MONTH', \n",
    "    'DAY_OF_MONTH', \n",
    "    'DAY_OF_WEEK', \n",
    "    'IS_US_HOLIDAY', \n",
    "    'origin_pagerank', \n",
    "    'dest_pagerank', \n",
    "    'prev_flight_arr_delay_clean', \n",
    "    'actual_to_crs_time_to_next_flight_diff_mins_clean'\n",
    "]\n",
    "\n",
    "cat_cols_to_use = [\n",
    "    'OP_UNIQUE_CARRIER_idx', \n",
    "    'ORIGIN_idx', \n",
    "    'DEST_idx', \n",
    "    'ORIGIN_STATE_ABR_idx', \n",
    "    'DEST_STATE_ABR_idx', \n",
    "    'ORIGIN_SIZE_idx', \n",
    "    # 'DEST_SIZE_idx',            # 0.0005% - REMOVED\n",
    "    'HourlyWindCardinalDirection_idx', \n",
    "    'CRS_DEP_TIME_BLOCK_idx', \n",
    "    'CRS_ARR_TIME_BLOCK_idx'\n",
    "]\n",
    "\n",
    "all_model_features = numeric_cols_to_use + cat_cols_to_use\n",
    "\n",
    "\n",
    "print(f\"FEATURE DEFINITIONS FOR MODEL\")\n",
    "print(f\"Numeric columns: {len(numeric_cols_to_use)}\")\n",
    "print(f\"Indexed categorical columns: {len(cat_cols_to_use)}\")\n",
    "print(f\"Total model features: {len(all_model_features)}\")\n",
    "\n",
    "missing_from_test = [c for c in all_model_features if c not in available_cols]\n",
    "present_in_test = [c for c in all_model_features if c in available_cols]\n",
    "\n",
    "print(f\"\\nPresent in test: {len(present_in_test)}/{len(all_model_features)}\")\n",
    "print(f\"Missing from test: {len(missing_from_test)}\")\n",
    "\n",
    "if missing_from_test:\n",
    "    print(f\"\\n⚠️ Missing features:\")\n",
    "    for col in missing_from_test:\n",
    "        print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b5c9e15-cb85-476b-b953-2f111f58c044",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Features Correlation Analysis"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: CORRELATION ANALYSIS - FIND REDUNDANT FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select only numeric features for correlation\n",
    "numeric_features_present = [col for col in numeric_cols_to_use if col in available_cols]\n",
    "\n",
    "# Convert to Pandas for correlation analysis (sample if dataset is large)\n",
    "sample_size = min(100000, df_train_master.count())\n",
    "df_sample = df_train_master.select(numeric_features_present + ['ARR_DEL15']).sample(\n",
    "    fraction=sample_size/df_train_master.count(), \n",
    "    seed=42\n",
    ").toPandas()\n",
    "\n",
    "# Correlation with target\n",
    "target_corr = df_sample[numeric_features_present].corrwith(df_sample['ARR_DEL15']).abs().sort_values(ascending=False)\n",
    "print(target_corr)\n",
    "\n",
    "# Find highly correlated feature pairs (redundant features)\n",
    "corr_matrix = df_sample[numeric_features_present].corr().abs()\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "redundant_pairs = []\n",
    "for column in upper_triangle.columns:\n",
    "    high_corr = upper_triangle[column][upper_triangle[column] > 0.85]  # 0.85+ is highly redundant\n",
    "    if len(high_corr) > 0:\n",
    "        for idx in high_corr.index:\n",
    "            redundant_pairs.append((column, idx, high_corr[idx]))\n",
    "\n",
    "print(f\"\\nHighly correlated feature pairs (>0.85) - consider removing one from each pair:\")\n",
    "if redundant_pairs:\n",
    "    for feat1, feat2, corr_val in redundant_pairs:\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr_val:.3f}\")\n",
    "else:\n",
    "    print(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cdf14bf-43e9-4ee7-8907-088f07ef8144",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Near Constant Analysis"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "print(\"ACTUAL FEATURE VARIABILITY:\")\n",
    "print(\"\\n✅ GOOD VARIABILITY (Keep these):\")\n",
    "good_features = [\n",
    "    \"DISTANCE: 1,574 distinct values - GOOD\",\n",
    "    \"prev_flight_arr_delay_clean: 1,446 distinct values - GOOD\", \n",
    "    \"actual_to_crs_time_to_next_flight_diff_mins_clean: 21,339 distinct values - EXCELLENT\",\n",
    "    \"HourlyAltimeterSetting: 394 distinct values - GOOD\",\n",
    "    \"ORIGIN/DEST coordinates: 354-355 distinct values - GOOD\",\n",
    "    \"origin/dest_pagerank: 354-355 distinct values - GOOD\",\n",
    "    \"ORIGIN/DEST_ELEVATION: 308 distinct values - ACCEPTABLE\",\n",
    "    \"Cloud heights: 117-135 distinct values - ACCEPTABLE\",\n",
    "]\n",
    "for feat in good_features:\n",
    "    print(f\"  • {feat}\")\n",
    "\n",
    "print(\"\\n⚠️  LIMITED VARIABILITY (Consider consolidating):\")\n",
    "limited_features = [\n",
    "    \"HourlyWindSpeed: 58 distinct values - ACCEPTABLE but limited\",\n",
    "    \"HourlyWindGustSpeed: 59 distinct values - ACCEPTABLE but limited\",\n",
    "    \"DAY_OF_MONTH: 31 distinct values - ACCEPTABLE\",\n",
    "    \"MONTH: 12 distinct values - ACCEPTABLE\",\n",
    "    \"DAY_OF_WEEK: 7 distinct values - ACCEPTABLE\",\n",
    "    \"overall_cloud_frac_0_1: 5 distinct values - LIMITED (but likely important)\",\n",
    "]\n",
    "for feat in limited_features:\n",
    "    print(f\"  • {feat}\")\n",
    "\n",
    "print(\"\\n❌ BINARY FEATURES (2 distinct values - 0/1 flags):\")\n",
    "binary_features = [\n",
    "    'has_few', 'has_sct', 'has_bkn', 'has_ovc',\n",
    "    'light', 'heavy', 'thunderstorm', 'rain_or_drizzle',\n",
    "    'freezing_conditions', 'snow', 'hail_or_ice',\n",
    "    'reduced_visibility', 'spatial_effects', 'unknown_precip',\n",
    "    'IS_US_HOLIDAY'\n",
    "]\n",
    "print(\"These are binary indicators - NOT near-constant, just binary by design!\")\n",
    "print(f\"  {len(binary_features)} binary features\")\n",
    "\n",
    "# The real question for binary features: what's the class balance?\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BINARY FEATURE CLASS BALANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"Checking if any binary features are extremely imbalanced...\\n\")\n",
    "\n",
    "imbalanced_features = []\n",
    "for col in binary_features:\n",
    "    if col in df_train_master.columns:\n",
    "        # Get value counts\n",
    "        counts = df_train_master.groupBy(col).count().collect()\n",
    "        if len(counts) == 2:\n",
    "            count_0 = [r['count'] for r in counts if r[col] == 0][0]\n",
    "            count_1 = [r['count'] for r in counts if r[col] == 1][0]\n",
    "            total = count_0 + count_1\n",
    "            \n",
    "            # Calculate minority class percentage\n",
    "            minority_pct = min(count_0, count_1) / total * 100\n",
    "            \n",
    "            # Flag if minority class is < 1%\n",
    "            if minority_pct < 1.0:\n",
    "                imbalanced_features.append({\n",
    "                    'feature': col,\n",
    "                    'minority_pct': minority_pct,\n",
    "                    'count_0': count_0,\n",
    "                    'count_1': count_1\n",
    "                })\n",
    "                print(f\"⚠️  {col}: {minority_pct:.3f}% minority class\")\n",
    "            elif minority_pct < 5.0:\n",
    "                print(f\"   {col}: {minority_pct:.2f}% minority class (somewhat rare)\")\n",
    "\n",
    "if imbalanced_features:\n",
    "    print(f\"\\n\uD83C\uDFAF HIGHLY IMBALANCED FEATURES (< 1% minority class): {len(imbalanced_features)}\")\n",
    "    print(\"These might not add much predictive power:\")\n",
    "    for feat in imbalanced_features:\n",
    "        print(f\"  • {feat['feature']}: {feat['minority_pct']:.3f}%\")\n",
    "else:\n",
    "    print(\"\\n✅ No extremely imbalanced binary features found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbbf5383-8c46-4c44-bfe6-f1414ee99435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cardinality Analysis for Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad8f3166-e23c-4291-bb83-2b37c2338d7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cardinality Check"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cardinality for 9 indexed categorical columns\n\nTest Data Cardinality\n\nColumn                                     Unique Values      Exceeds maxBins\n-----------------------------------------------------------------------------\nORIGIN_idx                                           347                 ✓ OK\nDEST_idx                                             347                 ✓ OK\nORIGIN_STATE_ABR_idx                                  49                 ✓ OK\nDEST_STATE_ABR_idx                                    49                 ✓ OK\nCRS_ARR_TIME_BLOCK_idx                                25                 ✓ OK\nCRS_DEP_TIME_BLOCK_idx                                24                 ✓ OK\nOP_UNIQUE_CARRIER_idx                                 17                 ✓ OK\nHourlyWindCardinalDirection_idx                       10                 ✓ OK\nORIGIN_SIZE_idx                                        3                 ✓ OK\n\nTraining Data Cardinality\n\nColumn                                     Unique Values      Exceeds maxBins\n-----------------------------------------------------------------------------\nORIGIN_idx                                           313                 ✓ OK\nDEST_idx                                             312                 ✓ OK\nORIGIN_STATE_ABR_idx                                  49                 ✓ OK\nDEST_STATE_ABR_idx                                    49                 ✓ OK\nCRS_ARR_TIME_BLOCK_idx                                25                 ✓ OK\nCRS_DEP_TIME_BLOCK_idx                                24                 ✓ OK\nOP_UNIQUE_CARRIER_idx                                 13                 ✓ OK\nHourlyWindCardinalDirection_idx                       10                 ✓ OK\nORIGIN_SIZE_idx                                        3                 ✓ OK\n"
     ]
    }
   ],
   "source": [
    "max_bins = 400\n",
    "\n",
    "cols_to_check = cat_cols_to_use.copy()\n",
    "\n",
    "print(f\"Checking cardinality for {len(cols_to_check)} indexed categorical columns\")\n",
    "\n",
    "print(f\"\\nTest Data Cardinality\")\n",
    "test_cardinality = []\n",
    "for col in cols_to_check:\n",
    "    if col in df_test_final.columns:\n",
    "        distinct_count = df_test_final.select(F.countDistinct(col)).first()[0]\n",
    "        test_cardinality.append((col, distinct_count))\n",
    "    else:\n",
    "        print(f\"Column not in test data: {col}\")\n",
    "\n",
    "print(f\"\\n{'Column':<40} {'Unique Values':>15} {'Exceeds maxBins':>20}\")\n",
    "print(\"-\" * 77)\n",
    "for col, count in sorted(test_cardinality, key=lambda x: x[1], reverse=True):\n",
    "    exceeds = \"⚠️ YES\" if count > max_bins else \"✓ OK\"\n",
    "    print(f\"{col:<40} {count:>15,} {exceeds:>20}\")\n",
    "\n",
    "print(f\"\\nTraining Data Cardinality\")\n",
    "train_cardinality = []\n",
    "for col in cols_to_check:\n",
    "    if col in df_sample.columns:\n",
    "        distinct_count = df_sample.select(F.countDistinct(col)).first()[0]\n",
    "        train_cardinality.append((col, distinct_count))\n",
    "    else:\n",
    "        print(f\"Column not in training data: {col}\")\n",
    "\n",
    "print(f\"\\n{'Column':<40} {'Unique Values':>15} {'Exceeds maxBins':>20}\")\n",
    "print(\"-\" * 77)\n",
    "for col, count in sorted(train_cardinality, key=lambda x: x[1], reverse=True):\n",
    "    exceeds = \"⚠️ YES\" if count > max_bins else \"✓ OK\"\n",
    "    print(f\"{col:<40} {count:>15,} {exceeds:>20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc07cd3-6a5d-4ec2-859b-796965f37b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d2b3465-e941-4fc9-a4bd-55a04f833562",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Confirmation on Column Names"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nUsing 26 numeric columns:\n['DISTANCE', 'ORIGIN_ELEVATION_FT', 'DEST_ELEVATION_FT', 'overall_cloud_frac_0_1', 'lowest_cloud_ft', 'highest_cloud_ft', 'has_ovc', 'HourlyAltimeterSetting', 'HourlyWindGustSpeed', 'HourlyWindSpeed', 'light', 'rain_or_drizzle', 'snow', 'reduced_visibility', 'ORIGIN_LAT', 'ORIGIN_LONG', 'DEST_LAT', 'DEST_LON', 'MONTH', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'IS_US_HOLIDAY', 'origin_pagerank', 'dest_pagerank', 'prev_flight_arr_delay_clean', 'actual_to_crs_time_to_next_flight_diff_mins_clean']\n\nUsing 9 categorical columns:\n['OP_UNIQUE_CARRIER_idx', 'ORIGIN_idx', 'DEST_idx', 'ORIGIN_STATE_ABR_idx', 'DEST_STATE_ABR_idx', 'ORIGIN_SIZE_idx', 'HourlyWindCardinalDirection_idx', 'CRS_DEP_TIME_BLOCK_idx', 'CRS_ARR_TIME_BLOCK_idx']\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nUsing {len(numeric_cols_to_use)} numeric columns:\")\n",
    "print(numeric_cols_to_use)\n",
    "print(f\"\\nUsing {len(cat_cols_to_use)} categorical columns:\")\n",
    "print(cat_cols_to_use)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e92c2def-59ad-46d0-a50f-0b988a58bbf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Function for Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0ba564-3185-4629-b5f8-2fa18dc51ddb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Plot PR-AUC Curve Function"
    }
   },
   "outputs": [],
   "source": [
    "def plot_pr(preds, title=\"Precision–Recall Curve\"):\n",
    "    # get (score, label)\n",
    "    rows = preds.select(\"probability\", \"ARR_DEL15\").collect()\n",
    "    data = [(float(r[\"probability\"][1]), float(r[\"ARR_DEL15\"])) for r in rows]\n",
    "\n",
    "    # sort descending by score\n",
    "    data.sort(key=lambda x: -x[0])\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    positives = sum(y for _, y in data)\n",
    "\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "\n",
    "    for score, label in data:\n",
    "        if label == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "        precision = tp / (tp + fp)\n",
    "        recall    = tp / positives\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(recalls, precisions)\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(title)\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc69a001-1342-4edc-a923-08e3aed1e1a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reference EVAL Function"
    }
   },
   "outputs": [],
   "source": [
    "def cv_eval(preds):\n",
    "    \"\"\"\n",
    "    Input: preds = Spark DF with prediction, ARR_DEL15, probability\n",
    "    Output: F2, PR-AUC, Precision, Recall, TP, FP, FN, TN\n",
    "    \"\"\"\n",
    "    rdd_preds_m = preds.select(\n",
    "        ['prediction', 'ARR_DEL15']\n",
    "    ).rdd.map(lambda row: (float(row['prediction']), float(row['ARR_DEL15'])))\n",
    "\n",
    "    rdd_preds_b = preds.select(\n",
    "        ['probability', 'ARR_DEL15']\n",
    "    ).rdd.map(lambda row: (float(row['probability'][1]), float(row['ARR_DEL15'])))\n",
    "\n",
    "    metrics_m = MulticlassMetrics(rdd_preds_m)\n",
    "    metrics_b = BinaryClassificationMetrics(rdd_preds_b)\n",
    "\n",
    "    precision = round(metrics_m.precision(1.0), 4)\n",
    "    recall = round(metrics_m.recall(1.0), 4)\n",
    "    F2 = round(metrics_m.fMeasure(label=1.0, beta=2.0), 4)\n",
    "    pr_auc = round(metrics_b.areaUnderPR, 4)\n",
    "\n",
    "    cm = metrics_m.confusionMatrix().toArray()\n",
    "\n",
    "    TN = int(cm[0, 0])\n",
    "    FP = int(cm[0, 1])\n",
    "    FN = int(cm[1, 0])\n",
    "    TP = int(cm[1, 1])\n",
    "\n",
    "    return F2, pr_auc, precision, recall, TP, FP, FN, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a85f1132-ff68-4f8a-8f65-9db75042a838",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Random Forest Function"
    }
   },
   "outputs": [],
   "source": [
    "def get_rf_model(params, cat_cols_to_use, numeric_cols_to_use, use_weights=False):\n",
    "    from pyspark.ml.classification import RandomForestClassifier\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import VectorAssembler\n",
    "    \n",
    "    final_assembler = VectorAssembler(\n",
    "        inputCols=numeric_cols_to_use + cat_cols_to_use,  \n",
    "        outputCol=\"features\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    rf_params = {\n",
    "        'labelCol': \"ARR_DEL15\",\n",
    "        'featuresCol': \"features\",\n",
    "        'maxDepth': params['maxDepth'],\n",
    "        'numTrees': params['numTrees'],\n",
    "        'maxBins': params.get('maxBins', 400),\n",
    "        'minInstancesPerNode': params.get('minInstancesPerNode', 1),\n",
    "        'featureSubsetStrategy': params.get('featureSubsetStrategy', 'auto'),\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    if use_weights:\n",
    "        rf_params['weightCol'] = 'weight'\n",
    "    \n",
    "    rf = RandomForestClassifier(**rf_params)\n",
    "    pipeline = Pipeline(stages=[final_assembler, rf])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e5ac8a-4217-402b-a949-e7d6f79dd60c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Windows Training Eval Function"
    }
   },
   "outputs": [],
   "source": [
    "def model_eval_on_folds(folds_input_path, param_grid, cat_cols_to_use, numeric_cols_to_use, \n",
    "                        N_SPLITS=N_SPLITS, plot_pr_curve_func=None, use_weights=False):\n",
    "    \"\"\"\n",
    "    Evaluate Random Forest model on pre-processed rolling windows\n",
    "    \n",
    "    Args:\n",
    "        folds_input_path: path to transform_rolling_window (already preprocessed)\n",
    "        param_grid: dict with RF parameters\n",
    "        cat_cols_to_use: filtered categorical columns\n",
    "        numeric_cols_to_use: filtered numeric columns\n",
    "        N_SPLITS: number of windows\n",
    "        plot_pr_curve_func: function to plot PR curve (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metrics and best parameters\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "    from itertools import product\n",
    "    import numpy as np\n",
    "    \n",
    "    # Generate parameter combinations\n",
    "    parameter_names = list(param_grid.keys())\n",
    "    parameter_values = list(param_grid.values())\n",
    "    parameters = list(product(*parameter_values))\n",
    "    \n",
    "    best_score = 0\n",
    "    best_parameters = None\n",
    "    best_fold_predictions = []\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"\\nEvaluating Random Forest Model\")\n",
    "    print(f\"Number of windows: {N_SPLITS}\")\n",
    "    print(f\"Parameter combinations to test: {len(parameters)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Loop through parameter combinations\n",
    "    for p in parameters:\n",
    "        param_print = {x[0]:x[1] for x in zip(parameter_names, p)}\n",
    "        print(f\"\\nTesting Parameters: {param_print}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        scores_f2 = []\n",
    "        scores_pr = []\n",
    "        scores_precision = []\n",
    "        scores_recall = []\n",
    "        confusion_list = []\n",
    "        fold_predictions = []\n",
    "        \n",
    "        # Loop through ALL windows\n",
    "        for i in range(N_SPLITS):\n",
    "            print(f\"  Window {i+1}:\")\n",
    "            train_path = f\"{folds_input_path}/window_{i+1}_train\"\n",
    "            val_path = f\"{folds_input_path}/window_{i+1}_val\"\n",
    "            # Load preprocessed data\n",
    "            train_df = spark.read.parquet(f\"{folds_input_path}/window_{i+1}_train\")\n",
    "            val_df = spark.read.parquet(f\"{folds_input_path}/window_{i+1}_val\")\n",
    "            print(f\"    Train path: {train_path}\")  # Added this line\n",
    "            print(f\"    Val path: {val_path}\")   \n",
    "            \n",
    "            print(f\"    Train samples: {train_df.count():,}\")\n",
    "            print(f\"    Val samples: {val_df.count():,}\")\n",
    "            \n",
    "            # Build and train model\n",
    "            pipeline = get_rf_model(param_print, cat_cols_to_use, numeric_cols_to_use, use_weights=False)\n",
    "            model = pipeline.fit(train_df)\n",
    "            preds = model.transform(val_df)\n",
    "            \n",
    "            # Compute metrics\n",
    "            f2, pr_auc, precision, recall, tp, fp, fn, tn = cv_eval(preds)\n",
    "            scores_f2.append(f2)\n",
    "            scores_pr.append(pr_auc)\n",
    "            scores_precision.append(precision)\n",
    "            scores_recall.append(recall)\n",
    "            confusion_list.append({\"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn})\n",
    "            fold_predictions.append((i+1, preds, f2, pr_auc, precision, recall, tp, fp, fn, tn))\n",
    "            \n",
    "            print(f\"    F2: {f2:.4f}, PR-AUC: {pr_auc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "            print(f\"    TP={tp}, FP={fp}, FN={fn}, TN={tn}\")\n",
    "        \n",
    "        # Calculate average scores for this parameter combination\n",
    "        avg_f2 = np.mean(scores_f2)\n",
    "        avg_pr = np.mean(scores_pr)\n",
    "        avg_precision = np.mean(scores_precision)\n",
    "        avg_recall = np.mean(scores_recall)\n",
    "        \n",
    "        print(f\"\\n  Average across {N_SPLITS} windows:\")\n",
    "        print(f\"    F2: {avg_f2:.4f} (±{np.std(scores_f2):.4f})\")\n",
    "        print(f\"    PR-AUC: {avg_pr:.4f} (±{np.std(scores_pr):.4f})\")\n",
    "        print(f\"    Precision: {avg_precision:.4f} (±{np.std(scores_precision):.4f})\")\n",
    "        print(f\"    Recall: {avg_recall:.4f} (±{np.std(scores_recall):.4f})\")\n",
    "        \n",
    "        # Update best parameters\n",
    "        if avg_f2 > best_score:\n",
    "            best_score = avg_f2\n",
    "            best_parameters = param_print\n",
    "            best_fold_predictions = fold_predictions\n",
    "            print(f\"  ✓ NEW BEST F2: {best_score:.4f}\")\n",
    "        \n",
    "        # Store results\n",
    "        all_results.append({\n",
    "            'parameters': param_print,\n",
    "            'f2_scores': scores_f2,\n",
    "            'pr_auc_scores': scores_pr,\n",
    "            'precision_scores': scores_precision,\n",
    "            'recall_scores': scores_recall,\n",
    "            'confusion': confusion_list,\n",
    "            'f2_mean': float(avg_f2),\n",
    "            'f2_std': float(np.std(scores_f2)),\n",
    "            'pr_auc_mean': float(avg_pr),\n",
    "            'pr_auc_std': float(np.std(scores_pr)),\n",
    "            'precision_mean': float(avg_precision),\n",
    "            'precision_std': float(np.std(scores_precision)),\n",
    "            'recall_mean': float(avg_recall),\n",
    "            'recall_std': float(np.std(scores_recall))\n",
    "        })\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    # Plot PR curves for all windows of best parameter combination\n",
    "    # if plot_pr_curve_func and best_fold_predictions:\n",
    "    #     print(f\"\\nPlotting PR curves for best parameters: {best_parameters}\")\n",
    "    #     for fold_num, preds, f2, pr_auc, precision, recall, tp, fp, fn, tn in best_fold_predictions:\n",
    "    #         plot_pr_curve_func(preds, title=f\"Best Model - Window {fold_num} (F2={f2:.4f}, PR-AUC={pr_auc:.4f}, P={precision:.4f}, R={recall:.4f})\")\n",
    "    \n",
    "    # Get best result\n",
    "    best_result = [r for r in all_results if r['f2_mean'] == best_score][0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL EVALUATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Best Parameters: {best_parameters}\")\n",
    "    print(f\"Best F2: {best_result['f2_mean']:.4f} (±{best_result['f2_std']:.4f})\")\n",
    "    print(f\"Best PR-AUC: {best_result['pr_auc_mean']:.4f} (±{best_result['pr_auc_std']:.4f})\")\n",
    "    print(f\"Best Precision: {best_result['precision_mean']:.4f} (±{best_result['precision_std']:.4f})\")\n",
    "    print(f\"Best Recall: {best_result['recall_mean']:.4f} (±{best_result['recall_std']:.4f})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'best_parameters': best_parameters,\n",
    "        'best_f2_mean': best_result['f2_mean'],\n",
    "        'best_f2_std': best_result['f2_std'],\n",
    "        'best_pr_auc_mean': best_result['pr_auc_mean'],\n",
    "        'best_pr_auc_std': best_result['pr_auc_std'],\n",
    "        'best_precision_mean': best_result['precision_mean'],\n",
    "        'best_precision_std': best_result['precision_std'],\n",
    "        'best_recall_mean': best_result['recall_mean'],\n",
    "        'best_recall_std': best_result['recall_std'],\n",
    "        'best_f2_scores': best_result['f2_scores'],\n",
    "        'best_pr_auc_scores': best_result['pr_auc_scores'],\n",
    "        'best_precision_scores': best_result['precision_scores'],\n",
    "        'best_recall_scores': best_result['recall_scores'],\n",
    "        'best_confusion': best_result['confusion'],\n",
    "        'all_results': all_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bd8ece-852c-4900-9815-70433e4e6a84",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Grid Search / Parameter Setting"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "  'maxDepth': [10],                  \n",
    "  'numTrees': [20],               \n",
    "  'maxBins': [400],                  \n",
    "  'minInstancesPerNode': [10],        \n",
    "  'featureSubsetStrategy': ['sqrt']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccc2840-d723-4fed-8218-d87eadbb498f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Model Training"
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "rf_results = model_eval_on_folds(\n",
    "    folds_input_path=folds_input_path,\n",
    "    param_grid=param_grid,\n",
    "    cat_cols_to_use=cat_cols_to_use,\n",
    "    numeric_cols_to_use=numeric_cols_to_use,\n",
    "    N_SPLITS=N_SPLITS,\n",
    "    plot_pr_curve_func=plot_pr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff673ea6-84f7-4bf0-81da-e8c73c54665b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model w/ ML Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf3eca1-cca1-4e43-ad00-205fe520d060",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ML Flow Location Setup"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment already exists or error creating: RESOURCE_ALREADY_EXISTS: Node named 'random_forest_basic' already exists\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='dbfs:/student-groups/Group_02_01/experiments/rf', creation_time=1764631967974, experiment_id='3915201981340333', last_update_time=1764631976232, lifecycle_stage='active', name='/Shared/Team_2_1/rf/random_forest_basic', tags={'mlflow.experiment.sourceName': '/Shared/Team_2_1/rf/random_forest_basic',\n",
       " 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n",
       " 'mlflow.ownerEmail': 'Team_2_1',\n",
       " 'mlflow.ownerId': '839106675862014'}>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "experiment_name = \"/Workspace/Shared/Team_2_1/rf/random_forest_basic\"\n",
    "\n",
    "try:\n",
    "    experiment_id = mlflow.create_experiment(\n",
    "        name=experiment_name,\n",
    "        artifact_location=\"dbfs:/student-groups/Group_02_01/experiments/rf\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Experiment already exists or error creating: {e}\")\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment:\n",
    "        experiment_id = experiment.experiment_id\n",
    "    else:\n",
    "        print(\"ERROR: Experiment doesn't exist and couldn't be created.\")\n",
    "        print(\"Make sure /Workspace/Shared/Team_2_1/rf/ folder exists in Databricks workspace\")\n",
    "        raise e\n",
    "\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd301c9-a39b-4d25-8a81-aac3ea382878",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Grid Search w/ ML Flow"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations: 9\nTotal runs (combinations × windows): 18\n\nTesting: {'maxDepth': 5, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 5, 'featureSubsetStrategy': 'sqrt'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:22:02 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmpco5qyjhx, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 1:\n    TRAIN  - F2=0.6322, PR-AUC=0.6575, Precision=0.6371, Recall=0.6310\n    VAL    - F2=0.5079, PR-AUC=0.3237, Precision=0.2971, Recall=0.6174\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:23:31 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmpzbtgmmz9, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 2:\n    TRAIN  - F2=0.6322, PR-AUC=0.6702, Precision=0.6454, Recall=0.6290\n    VAL    - F2=0.5048, PR-AUC=0.3248, Precision=0.3009, Recall=0.6078\n  Average: F2=0.5064 (±0.0015), PR-AUC=0.3242, Precision=0.2990, Recall=0.6126\n  ✓ NEW BEST!\n\nTesting: {'maxDepth': 5, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 10, 'featureSubsetStrategy': 'sqrt'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:24:50 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmp59g8t1ll, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 1:\n    TRAIN  - F2=0.6322, PR-AUC=0.6575, Precision=0.6371, Recall=0.6310\n    VAL    - F2=0.5079, PR-AUC=0.3237, Precision=0.2971, Recall=0.6174\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:26:17 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmpshdtownc, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 2:\n    TRAIN  - F2=0.6322, PR-AUC=0.6702, Precision=0.6454, Recall=0.6290\n    VAL    - F2=0.5048, PR-AUC=0.3248, Precision=0.3009, Recall=0.6078\n  Average: F2=0.5064 (±0.0015), PR-AUC=0.3242, Precision=0.2990, Recall=0.6126\n\nTesting: {'maxDepth': 5, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 20, 'featureSubsetStrategy': 'sqrt'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:27:37 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmpe33mpss5, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 1:\n    TRAIN  - F2=0.6322, PR-AUC=0.6575, Precision=0.6371, Recall=0.6310\n    VAL    - F2=0.5079, PR-AUC=0.3237, Precision=0.2971, Recall=0.6174\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:29:04 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmp71tcnwcp, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 2:\n    TRAIN  - F2=0.6322, PR-AUC=0.6702, Precision=0.6454, Recall=0.6290\n    VAL    - F2=0.5048, PR-AUC=0.3248, Precision=0.3009, Recall=0.6078\n  Average: F2=0.5064 (±0.0015), PR-AUC=0.3242, Precision=0.2990, Recall=0.6126\n\nTesting: {'maxDepth': 8, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 5, 'featureSubsetStrategy': 'sqrt'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:30:40 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmp5_c_vaoo, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 1:\n    TRAIN  - F2=0.6591, PR-AUC=0.6768, Precision=0.6454, Recall=0.6626\n    VAL    - F2=0.5286, PR-AUC=0.3450, Precision=0.3039, Recall=0.6485\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:32:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmp3azsbocp, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 2:\n    TRAIN  - F2=0.6649, PR-AUC=0.6888, Precision=0.6538, Recall=0.6677\n    VAL    - F2=0.5251, PR-AUC=0.3403, Precision=0.3069, Recall=0.6386\n  Average: F2=0.5269 (±0.0017), PR-AUC=0.3427, Precision=0.3054, Recall=0.6435\n  ✓ NEW BEST!\n\nTesting: {'maxDepth': 8, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 10, 'featureSubsetStrategy': 'sqrt'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:34:07 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmprlwyx1bx, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 1:\n    TRAIN  - F2=0.6639, PR-AUC=0.6760, Precision=0.6440, Recall=0.6691\n    VAL    - F2=0.5309, PR-AUC=0.3444, Precision=0.3026, Recall=0.6543\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:35:57 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmpj9pcy0w8, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 2:\n    TRAIN  - F2=0.6638, PR-AUC=0.6892, Precision=0.6534, Recall=0.6664\n    VAL    - F2=0.5242, PR-AUC=0.3417, Precision=0.3068, Recall=0.6370\n  Average: F2=0.5275 (±0.0034), PR-AUC=0.3430, Precision=0.3047, Recall=0.6457\n  ✓ NEW BEST!\n\nTesting: {'maxDepth': 8, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 20, 'featureSubsetStrategy': 'sqrt'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:37:35 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmpfkl6_ls9, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 1:\n    TRAIN  - F2=0.6641, PR-AUC=0.6772, Precision=0.6436, Recall=0.6695\n    VAL    - F2=0.5302, PR-AUC=0.3451, Precision=0.3020, Recall=0.6538\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:39:25 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmp2ulfurgi, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 2:\n    TRAIN  - F2=0.6650, PR-AUC=0.6885, Precision=0.6528, Recall=0.6681\n    VAL    - F2=0.5248, PR-AUC=0.3409, Precision=0.3059, Recall=0.6392\n  Average: F2=0.5275 (±0.0027), PR-AUC=0.3430, Precision=0.3039, Recall=0.6465\n\nTesting: {'maxDepth': 10, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 5, 'featureSubsetStrategy': 'sqrt'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:41:26 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmp3bj0mfna, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 1:\n    TRAIN  - F2=0.6700, PR-AUC=0.6892, Precision=0.6522, Recall=0.6746\n    VAL    - F2=0.5354, PR-AUC=0.3558, Precision=0.3085, Recall=0.6560\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:43:40 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmp3foy79h4, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 2:\n    TRAIN  - F2=0.6745, PR-AUC=0.7016, Precision=0.6611, Recall=0.6779\n    VAL    - F2=0.5315, PR-AUC=0.3516, Precision=0.3109, Recall=0.6462\n  Average: F2=0.5334 (±0.0020), PR-AUC=0.3537, Precision=0.3097, Recall=0.6511\n  ✓ NEW BEST!\n\nTesting: {'maxDepth': 10, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 10, 'featureSubsetStrategy': 'sqrt'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:45:42 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmpj92fs7e5, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 1:\n    TRAIN  - F2=0.6729, PR-AUC=0.6897, Precision=0.6510, Recall=0.6786\n    VAL    - F2=0.5375, PR-AUC=0.3572, Precision=0.3076, Recall=0.6610\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:47:55 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmpmqd3k0in, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 2:\n    TRAIN  - F2=0.6753, PR-AUC=0.7007, Precision=0.6605, Recall=0.6791\n    VAL    - F2=0.5321, PR-AUC=0.3511, Precision=0.3105, Recall=0.6477\n  Average: F2=0.5348 (±0.0027), PR-AUC=0.3542, Precision=0.3090, Recall=0.6543\n  ✓ NEW BEST!\n\nTesting: {'maxDepth': 10, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 20, 'featureSubsetStrategy': 'sqrt'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:49:55 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmp89kh2vuo, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 1:\n    TRAIN  - F2=0.6689, PR-AUC=0.6897, Precision=0.6521, Recall=0.6732\n    VAL    - F2=0.5348, PR-AUC=0.3572, Precision=0.3085, Recall=0.6548\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 20:52:08 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmp2u1lzftb, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 2:\n    TRAIN  - F2=0.6754, PR-AUC=0.7009, Precision=0.6608, Recall=0.6791\n    VAL    - F2=0.5320, PR-AUC=0.3514, Precision=0.3110, Recall=0.6470\n  Average: F2=0.5334 (±0.0014), PR-AUC=0.3543, Precision=0.3097, Recall=0.6509\n\n================================================================================\nGRID SEARCH COMPLETE\n================================================================================\nBest Parameters: {'maxDepth': 10, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 10, 'featureSubsetStrategy': 'sqrt'}\nBest F2: 0.5348\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "experiment_name_mlflow = \"/Workspace/Shared/Team_2_1/rf/random_forest_grid_search\"\n",
    "\n",
    "try:\n",
    "    experiment_id_mlflow = mlflow.create_experiment(\n",
    "        name=experiment_name_mlflow,\n",
    "        artifact_location=\"dbfs:/student-groups/Group_02_01/experiments/rf\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    experiment_mlflow = mlflow.get_experiment_by_name(experiment_name_mlflow)\n",
    "    if experiment_mlflow:\n",
    "        experiment_id_mlflow = experiment_mlflow.experiment_id\n",
    "\n",
    "mlflow.set_experiment(experiment_name_mlflow)\n",
    "\n",
    "param_grid_mlflow = {\n",
    "    'maxDepth': [5,8,10],\n",
    "    'numTrees': [20],\n",
    "    'maxBins': [400],\n",
    "    'minInstancesPerNode': [5,10,20],\n",
    "    'featureSubsetStrategy': ['sqrt']\n",
    "}\n",
    "\n",
    "parameter_names_mlflow = list(param_grid_mlflow.keys())\n",
    "parameter_values_mlflow = list(param_grid_mlflow.values())\n",
    "parameters_mlflow = list(product(*parameter_values_mlflow))\n",
    "\n",
    "print(f\"Total parameter combinations: {len(parameters_mlflow)}\")\n",
    "print(f\"Total runs (combinations × windows): {len(parameters_mlflow) * N_SPLITS}\")\n",
    "\n",
    "best_f2_mlflow = 0\n",
    "best_params_mlflow = None\n",
    "\n",
    "for p in parameters_mlflow:\n",
    "    param_dict_mlflow = {x[0]:x[1] for x in zip(parameter_names_mlflow, p)}\n",
    "    print(f\"\\nTesting: {param_dict_mlflow}\")\n",
    "    \n",
    "    f2_scores_mlflow = []\n",
    "    pr_scores_mlflow = []\n",
    "    precision_scores_mlflow = []\n",
    "    recall_scores_mlflow = []\n",
    "    val_f2_scores = []\n",
    "    val_pr_scores = []\n",
    "    val_precision_scores = []\n",
    "    val_recall_scores = []\n",
    "    \n",
    "    for i in range(N_SPLITS):\n",
    "        train_path_mlflow = f\"{folds_input_path}/window_{i+1}_train\"\n",
    "        val_path_mlflow = f\"{folds_input_path}/window_{i+1}_val\"\n",
    "        \n",
    "        train_df_mlflow = spark.read.parquet(train_path_mlflow)\n",
    "        val_df_mlflow = spark.read.parquet(val_path_mlflow)\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"d{param_dict_mlflow['maxDepth']}_t{param_dict_mlflow['numTrees']}_w{i+1}\"):\n",
    "            pipeline_mlflow = get_rf_model(param_dict_mlflow, cat_cols_to_use, numeric_cols_to_use, use_weights=False)\n",
    "            model_mlflow = pipeline_mlflow.fit(train_df_mlflow)\n",
    "            preds_mlflow = model_mlflow.transform(val_df_mlflow)\n",
    "            \n",
    "            # Metrics for validation data\n",
    "            val_f2, val_pr_auc, val_precision, val_recall, val_tp, val_fp, val_fn, val_tn = cv_eval(preds_mlflow)\n",
    "            val_f2_scores.append(val_f2)\n",
    "            val_pr_scores.append(val_pr_auc)\n",
    "            val_precision_scores.append(val_precision)\n",
    "            val_recall_scores.append(val_recall)\n",
    "            \n",
    "            # Metrics for training data\n",
    "            train_preds = model_mlflow.transform(train_df_mlflow)\n",
    "            train_f2, train_pr_auc, train_precision, train_recall, train_tp, train_fp, train_fn, train_tn = cv_eval(train_preds)\n",
    "            f2_scores_mlflow.append(train_f2)\n",
    "            pr_scores_mlflow.append(train_pr_auc)\n",
    "            precision_scores_mlflow.append(train_precision)\n",
    "            recall_scores_mlflow.append(train_recall)\n",
    "            \n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"maxDepth\", param_dict_mlflow['maxDepth'])\n",
    "            mlflow.log_param(\"numTrees\", param_dict_mlflow['numTrees'])\n",
    "            mlflow.log_param(\"maxBins\", param_dict_mlflow['maxBins'])\n",
    "            mlflow.log_param(\"minInstancesPerNode\", param_dict_mlflow['minInstancesPerNode'])\n",
    "            mlflow.log_param(\"featureSubsetStrategy\", param_dict_mlflow['featureSubsetStrategy'])\n",
    "            mlflow.log_param(\"window\", i+1)\n",
    "            \n",
    "            # Log training metrics\n",
    "            mlflow.log_metric(\"train_f2_score\", train_f2)\n",
    "            mlflow.log_metric(\"train_pr_auc\", train_pr_auc)\n",
    "            mlflow.log_metric(\"train_precision\", train_precision)\n",
    "            mlflow.log_metric(\"train_recall\", train_recall)\n",
    "            mlflow.log_metric(\"train_true_positives\", train_tp)\n",
    "            mlflow.log_metric(\"train_false_positives\", train_fp)\n",
    "            mlflow.log_metric(\"train_false_negatives\", train_fn)\n",
    "            mlflow.log_metric(\"train_true_negatives\", train_tn)\n",
    "            \n",
    "            # Log validation metrics\n",
    "            mlflow.log_metric(\"val_f2_score\", val_f2)\n",
    "            mlflow.log_metric(\"val_pr_auc\", val_pr_auc)\n",
    "            mlflow.log_metric(\"val_precision\", val_precision)\n",
    "            mlflow.log_metric(\"val_recall\", val_recall)\n",
    "            mlflow.log_metric(\"val_true_positives\", val_tp)\n",
    "            mlflow.log_metric(\"val_false_positives\", val_fp)\n",
    "            mlflow.log_metric(\"val_false_negatives\", val_fn)\n",
    "            mlflow.log_metric(\"val_true_negatives\", val_tn)\n",
    "            \n",
    "            mlflow.spark.log_model(model_mlflow, artifact_path=\"rf_model\")\n",
    "            \n",
    "            print(f\"  Window {i+1}:\")\n",
    "            print(f\"    TRAIN  - F2={train_f2:.4f}, PR-AUC={train_pr_auc:.4f}, Precision={train_precision:.4f}, Recall={train_recall:.4f}\")\n",
    "            print(f\"    VAL    - F2={val_f2:.4f}, PR-AUC={val_pr_auc:.4f}, Precision={val_precision:.4f}, Recall={val_recall:.4f}\")\n",
    "    \n",
    "    avg_f2_mlflow = np.mean(val_f2_scores)\n",
    "    avg_pr_mlflow = np.mean(val_pr_scores)\n",
    "    avg_precision_mlflow = np.mean(val_precision_scores)\n",
    "    avg_recall_mlflow = np.mean(val_recall_scores)\n",
    "    \n",
    "    print(f\"  Average: F2={avg_f2_mlflow:.4f} (±{np.std(val_f2_scores):.4f}), PR-AUC={avg_pr_mlflow:.4f}, Precision={avg_precision_mlflow:.4f}, Recall={avg_recall_mlflow:.4f}\")\n",
    "    \n",
    "    if avg_f2_mlflow > best_f2_mlflow:\n",
    "        best_f2_mlflow = avg_f2_mlflow\n",
    "        best_params_mlflow = param_dict_mlflow\n",
    "        print(f\"  ✓ NEW BEST!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRID SEARCH COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best Parameters: {best_params_mlflow}\")\n",
    "print(f\"Best F2: {best_f2_mlflow:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d44cc48-4a00-44e4-9922-16adf73bf1d8",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764959798870}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "ML Flow Summary"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runs exported: 25\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>run_id</th><th>maxDepth</th><th>numTrees</th><th>maxBins</th><th>minInstancesPerNode</th><th>featureSubsetStrategy</th><th>window</th><th>train_f2_score</th><th>train_pr_auc</th><th>train_precision</th><th>train_recall</th><th>train_true_positives</th><th>train_false_positives</th><th>train_false_negatives</th><th>train_true_negatives</th><th>val_f2_score</th><th>val_pr_auc</th><th>val_precision</th><th>val_recall</th><th>val_true_positives</th><th>val_false_positives</th><th>val_false_negatives</th><th>val_true_negatives</th><th>wall_time_sec</th></tr></thead><tbody><tr><td>13cd55668ef743608fd8b5957fb55a70</td><td>10</td><td>20</td><td>400</td><td>5</td><td>sqrt</td><td>1</td><td>0.6745</td><td>0.691</td><td>0.6501</td><td>0.6808</td><td>1380375.0</td><td>742936.0</td><td>647104.0</td><td>1286433.0</td><td>0.539</td><td>0.3578</td><td>0.3076</td><td>0.6638</td><td>683921.0</td><td>1539408.0</td><td>346464.0</td><td>3002226.0</td><td>131.091</td></tr><tr><td>e82dd769a9be45d2a5276b0bb7cfa494</td><td>10</td><td>20</td><td>400</td><td>10</td><td>sqrt</td><td>1</td><td>0.6729</td><td>0.6897</td><td>0.651</td><td>0.6786</td><td>1375938.0</td><td>737800.0</td><td>651541.0</td><td>1291569.0</td><td>0.5375</td><td>0.3572</td><td>0.3076</td><td>0.661</td><td>681119.0</td><td>1533284.0</td><td>349266.0</td><td>3008350.0</td><td>120.113</td></tr><tr><td>d68ab5ac105742ac9a364f9ff188a208</td><td>10</td><td>20</td><td>400</td><td>5</td><td>sqrt</td><td>1</td><td>0.67</td><td>0.6892</td><td>0.6522</td><td>0.6746</td><td>1367772.0</td><td>729329.0</td><td>659707.0</td><td>1300040.0</td><td>0.5354</td><td>0.3558</td><td>0.3085</td><td>0.656</td><td>675931.0</td><td>1514899.0</td><td>354454.0</td><td>3026735.0</td><td>119.443</td></tr><tr><td>619b54481cc247a7a6a454a53b8e07c4</td><td>10</td><td>20</td><td>400</td><td>20</td><td>sqrt</td><td>1</td><td>0.6689</td><td>0.6897</td><td>0.6521</td><td>0.6732</td><td>1364968.0</td><td>728125.0</td><td>662511.0</td><td>1301244.0</td><td>0.5348</td><td>0.3572</td><td>0.3085</td><td>0.6548</td><td>674734.0</td><td>1512493.0</td><td>355651.0</td><td>3029141.0</td><td>119.287</td></tr><tr><td>68257ba2a0f541baa00265dffe975508</td><td>10</td><td>20</td><td>400</td><td>5</td><td>sqrt</td><td>2</td><td>0.6777</td><td>0.7022</td><td>0.6613</td><td>0.682</td><td>1359390.0</td><td>696237.0</td><td>633898.0</td><td>1298991.0</td><td>0.5323</td><td>0.3501</td><td>0.3097</td><td>0.6489</td><td>874465.0</td><td>1948950.0</td><td>473073.0</td><td>3789677.0</td><td>220.107</td></tr><tr><td>909e9b223baf4edea7ffb75f714b6c27</td><td>10</td><td>20</td><td>400</td><td>10</td><td>sqrt</td><td>2</td><td>0.6753</td><td>0.7007</td><td>0.6605</td><td>0.6791</td><td>1353646.0</td><td>695867.0</td><td>639642.0</td><td>1299361.0</td><td>0.5321</td><td>0.3511</td><td>0.3105</td><td>0.6477</td><td>872838.0</td><td>1938398.0</td><td>474700.0</td><td>3800229.0</td><td>130.828</td></tr><tr><td>9b098e8663a64d91b4bffe257801313e</td><td>10</td><td>20</td><td>400</td><td>5</td><td>sqrt</td><td>2</td><td>0.6772</td><td>0.702</td><td>0.6605</td><td>0.6816</td><td>1358545.0</td><td>698340.0</td><td>634743.0</td><td>1296888.0</td><td>0.5321</td><td>0.3503</td><td>0.3091</td><td>0.6492</td><td>874768.0</td><td>1955257.0</td><td>472770.0</td><td>3783370.0</td><td>145.111</td></tr><tr><td>656e52ab333740d1b26371eaa55ff3b3</td><td>10</td><td>20</td><td>400</td><td>20</td><td>sqrt</td><td>2</td><td>0.6754</td><td>0.7009</td><td>0.6608</td><td>0.6791</td><td>1353639.0</td><td>694935.0</td><td>639649.0</td><td>1300293.0</td><td>0.532</td><td>0.3514</td><td>0.311</td><td>0.647</td><td>871863.0</td><td>1931769.0</td><td>475675.0</td><td>3806858.0</td><td>130.762</td></tr><tr><td>f99c0a4821b447caaaebe55394134a1e</td><td>10</td><td>20</td><td>400</td><td>5</td><td>sqrt</td><td>2</td><td>0.6745</td><td>0.7016</td><td>0.6611</td><td>0.6779</td><td>1351259.0</td><td>692809.0</td><td>642029.0</td><td>1302419.0</td><td>0.5315</td><td>0.3516</td><td>0.3109</td><td>0.6462</td><td>870743.0</td><td>1930320.0</td><td>476795.0</td><td>3808307.0</td><td>132.651</td></tr><tr><td>1e44450e1242446bb31a6cccad7a3162</td><td>8</td><td>20</td><td>400</td><td>10</td><td>sqrt</td><td>1</td><td>0.6639</td><td>0.676</td><td>0.644</td><td>0.6691</td><td>1356610.0</td><td>750024.0</td><td>670869.0</td><td>1279345.0</td><td>0.5309</td><td>0.3444</td><td>0.3026</td><td>0.6543</td><td>674144.0</td><td>1553841.0</td><td>356241.0</td><td>2987793.0</td><td>95.801</td></tr><tr><td>5eb3b45682194121af7ca5e824503419</td><td>8</td><td>20</td><td>400</td><td>20</td><td>sqrt</td><td>1</td><td>0.6641</td><td>0.6772</td><td>0.6436</td><td>0.6695</td><td>1357344.0</td><td>751511.0</td><td>670135.0</td><td>1277858.0</td><td>0.5302</td><td>0.3451</td><td>0.302</td><td>0.6538</td><td>673670.0</td><td>1557200.0</td><td>356715.0</td><td>2984434.0</td><td>96.68</td></tr><tr><td>8baa6519a6704f0fb4b7e1e6ebace182</td><td>5</td><td>20</td><td>400</td><td>5</td><td>sqrt</td><td>1</td><td>0.6692</td><td>0.6597</td><td>0.6266</td><td>0.6808</td><td>1380275.0</td><td>822500.0</td><td>647204.0</td><td>1206869.0</td><td>0.529</td><td>0.3268</td><td>0.2871</td><td>0.6701</td><td>690491.0</td><td>1714199.0</td><td>339894.0</td><td>2827435.0</td><td>105.368</td></tr><tr><td>694086d794af4bca8b38e03438939f09</td><td>8</td><td>20</td><td>400</td><td>5</td><td>sqrt</td><td>1</td><td>0.6591</td><td>0.6768</td><td>0.6454</td><td>0.6626</td><td>1343504.0</td><td>738036.0</td><td>683975.0</td><td>1291333.0</td><td>0.5286</td><td>0.345</td><td>0.3039</td><td>0.6485</td><td>668170.0</td><td>1530316.0</td><td>362215.0</td><td>3011318.0</td><td>95.643</td></tr><tr><td>eb5bea983406403c8503168c98f84a75</td><td>8</td><td>20</td><td>400</td><td>5</td><td>sqrt</td><td>2</td><td>0.6649</td><td>0.6888</td><td>0.6538</td><td>0.6677</td><td>1330897.0</td><td>704608.0</td><td>662391.0</td><td>1290620.0</td><td>0.5251</td><td>0.3403</td><td>0.3069</td><td>0.6386</td><td>860561.0</td><td>1943758.0</td><td>486977.0</td><td>3794869.0</td><td>108.11</td></tr><tr><td>2b777789129641608738b551b0643539</td><td>8</td><td>20</td><td>400</td><td>20</td><td>sqrt</td><td>2</td><td>0.665</td><td>0.6885</td><td>0.6528</td><td>0.6681</td><td>1331759.0</td><td>708431.0</td><td>661529.0</td><td>1286797.0</td><td>0.5248</td><td>0.3409</td><td>0.3059</td><td>0.6392</td><td>861304.0</td><td>1954681.0</td><td>486234.0</td><td>3783946.0</td><td>108.52</td></tr><tr><td>0ec23fff330b4203ad99bb7ce378f339</td><td>8</td><td>20</td><td>400</td><td>10</td><td>sqrt</td><td>2</td><td>0.6638</td><td>0.6892</td><td>0.6534</td><td>0.6664</td><td>1328340.0</td><td>704666.0</td><td>664948.0</td><td>1290562.0</td><td>0.5242</td><td>0.3417</td><td>0.3068</td><td>0.637</td><td>858416.0</td><td>1939274.0</td><td>489122.0</td><td>3799353.0</td><td>107.774</td></tr><tr><td>ba29b9e91c2946038c382c777a250f7b</td><td>5</td><td>20</td><td>400</td><td>5</td><td>sqrt</td><td>2</td><td>0.6506</td><td>0.674</td><td>0.6463</td><td>0.6517</td><td>1298942.0</td><td>710894.0</td><td>694346.0</td><td>1284334.0</td><td>0.5152</td><td>0.3273</td><td>0.2999</td><td>0.6279</td><td>846179.0</td><td>1975042.0</td><td>501359.0</td><td>3763585.0</td><td>97.612</td></tr><tr><td>ee55c6f9015047fda83e3db9a7383e8c</td><td>5</td><td>20</td><td>400</td><td>20</td><td>sqrt</td><td>1</td><td>0.6322</td><td>0.6575</td><td>0.6371</td><td>0.631</td><td>1279356.0</td><td>728838.0</td><td>748123.0</td><td>1300531.0</td><td>0.5079</td><td>0.3237</td><td>0.2971</td><td>0.6174</td><td>636148.0</td><td>1504748.0</td><td>394237.0</td><td>3036886.0</td><td>78.205</td></tr><tr><td>e05ca267dbd34db290072953056e027f</td><td>5</td><td>20</td><td>400</td><td>10</td><td>sqrt</td><td>1</td><td>0.6322</td><td>0.6575</td><td>0.6371</td><td>0.631</td><td>1279356.0</td><td>728838.0</td><td>748123.0</td><td>1300531.0</td><td>0.5079</td><td>0.3237</td><td>0.2971</td><td>0.6174</td><td>636148.0</td><td>1504748.0</td><td>394237.0</td><td>3036886.0</td><td>76.735</td></tr><tr><td>4c7339f5a0b840efaf23e371862f1df9</td><td>5</td><td>20</td><td>400</td><td>5</td><td>sqrt</td><td>1</td><td>0.6322</td><td>0.6575</td><td>0.6371</td><td>0.631</td><td>1279356.0</td><td>728838.0</td><td>748123.0</td><td>1300531.0</td><td>0.5079</td><td>0.3237</td><td>0.2971</td><td>0.6174</td><td>636148.0</td><td>1504748.0</td><td>394237.0</td><td>3036886.0</td><td>78.592</td></tr><tr><td>9434b2ae9a05476ab1497e8da8ebbb9f</td><td>5</td><td>20</td><td>400</td><td>20</td><td>sqrt</td><td>2</td><td>0.6322</td><td>0.6702</td><td>0.6454</td><td>0.629</td><td>1253843.0</td><td>688981.0</td><td>739445.0</td><td>1306247.0</td><td>0.5048</td><td>0.3248</td><td>0.3009</td><td>0.6078</td><td>819037.0</td><td>1903093.0</td><td>528501.0</td><td>3835534.0</td><td>84.899</td></tr><tr><td>2f5f43c7d39b4bb796eccd6ec81f4550</td><td>5</td><td>20</td><td>400</td><td>10</td><td>sqrt</td><td>2</td><td>0.6322</td><td>0.6702</td><td>0.6454</td><td>0.629</td><td>1253843.0</td><td>688981.0</td><td>739445.0</td><td>1306247.0</td><td>0.5048</td><td>0.3248</td><td>0.3009</td><td>0.6078</td><td>819037.0</td><td>1903093.0</td><td>528501.0</td><td>3835534.0</td><td>85.767</td></tr><tr><td>247a1d327011459dba90b4d8da9cc2e3</td><td>5</td><td>20</td><td>400</td><td>5</td><td>sqrt</td><td>2</td><td>0.6322</td><td>0.6702</td><td>0.6454</td><td>0.629</td><td>1253843.0</td><td>688981.0</td><td>739445.0</td><td>1306247.0</td><td>0.5048</td><td>0.3248</td><td>0.3009</td><td>0.6078</td><td>819037.0</td><td>1903093.0</td><td>528501.0</td><td>3835534.0</td><td>87.589</td></tr><tr><td>e70461b151284087a2fca0f923054ec8</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>107.05</td></tr><tr><td>24b9141d488941318e768b5615687abb</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>15.568</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "13cd55668ef743608fd8b5957fb55a70",
         "10",
         "20",
         "400",
         "5",
         "sqrt",
         "1",
         0.6745,
         0.691,
         0.6501,
         0.6808,
         1380375.0,
         742936.0,
         647104.0,
         1286433.0,
         0.539,
         0.3578,
         0.3076,
         0.6638,
         683921.0,
         1539408.0,
         346464.0,
         3002226.0,
         131.091
        ],
        [
         "e82dd769a9be45d2a5276b0bb7cfa494",
         "10",
         "20",
         "400",
         "10",
         "sqrt",
         "1",
         0.6729,
         0.6897,
         0.651,
         0.6786,
         1375938.0,
         737800.0,
         651541.0,
         1291569.0,
         0.5375,
         0.3572,
         0.3076,
         0.661,
         681119.0,
         1533284.0,
         349266.0,
         3008350.0,
         120.113
        ],
        [
         "d68ab5ac105742ac9a364f9ff188a208",
         "10",
         "20",
         "400",
         "5",
         "sqrt",
         "1",
         0.67,
         0.6892,
         0.6522,
         0.6746,
         1367772.0,
         729329.0,
         659707.0,
         1300040.0,
         0.5354,
         0.3558,
         0.3085,
         0.656,
         675931.0,
         1514899.0,
         354454.0,
         3026735.0,
         119.443
        ],
        [
         "619b54481cc247a7a6a454a53b8e07c4",
         "10",
         "20",
         "400",
         "20",
         "sqrt",
         "1",
         0.6689,
         0.6897,
         0.6521,
         0.6732,
         1364968.0,
         728125.0,
         662511.0,
         1301244.0,
         0.5348,
         0.3572,
         0.3085,
         0.6548,
         674734.0,
         1512493.0,
         355651.0,
         3029141.0,
         119.287
        ],
        [
         "68257ba2a0f541baa00265dffe975508",
         "10",
         "20",
         "400",
         "5",
         "sqrt",
         "2",
         0.6777,
         0.7022,
         0.6613,
         0.682,
         1359390.0,
         696237.0,
         633898.0,
         1298991.0,
         0.5323,
         0.3501,
         0.3097,
         0.6489,
         874465.0,
         1948950.0,
         473073.0,
         3789677.0,
         220.107
        ],
        [
         "909e9b223baf4edea7ffb75f714b6c27",
         "10",
         "20",
         "400",
         "10",
         "sqrt",
         "2",
         0.6753,
         0.7007,
         0.6605,
         0.6791,
         1353646.0,
         695867.0,
         639642.0,
         1299361.0,
         0.5321,
         0.3511,
         0.3105,
         0.6477,
         872838.0,
         1938398.0,
         474700.0,
         3800229.0,
         130.828
        ],
        [
         "9b098e8663a64d91b4bffe257801313e",
         "10",
         "20",
         "400",
         "5",
         "sqrt",
         "2",
         0.6772,
         0.702,
         0.6605,
         0.6816,
         1358545.0,
         698340.0,
         634743.0,
         1296888.0,
         0.5321,
         0.3503,
         0.3091,
         0.6492,
         874768.0,
         1955257.0,
         472770.0,
         3783370.0,
         145.111
        ],
        [
         "656e52ab333740d1b26371eaa55ff3b3",
         "10",
         "20",
         "400",
         "20",
         "sqrt",
         "2",
         0.6754,
         0.7009,
         0.6608,
         0.6791,
         1353639.0,
         694935.0,
         639649.0,
         1300293.0,
         0.532,
         0.3514,
         0.311,
         0.647,
         871863.0,
         1931769.0,
         475675.0,
         3806858.0,
         130.762
        ],
        [
         "f99c0a4821b447caaaebe55394134a1e",
         "10",
         "20",
         "400",
         "5",
         "sqrt",
         "2",
         0.6745,
         0.7016,
         0.6611,
         0.6779,
         1351259.0,
         692809.0,
         642029.0,
         1302419.0,
         0.5315,
         0.3516,
         0.3109,
         0.6462,
         870743.0,
         1930320.0,
         476795.0,
         3808307.0,
         132.651
        ],
        [
         "1e44450e1242446bb31a6cccad7a3162",
         "8",
         "20",
         "400",
         "10",
         "sqrt",
         "1",
         0.6639,
         0.676,
         0.644,
         0.6691,
         1356610.0,
         750024.0,
         670869.0,
         1279345.0,
         0.5309,
         0.3444,
         0.3026,
         0.6543,
         674144.0,
         1553841.0,
         356241.0,
         2987793.0,
         95.801
        ],
        [
         "5eb3b45682194121af7ca5e824503419",
         "8",
         "20",
         "400",
         "20",
         "sqrt",
         "1",
         0.6641,
         0.6772,
         0.6436,
         0.6695,
         1357344.0,
         751511.0,
         670135.0,
         1277858.0,
         0.5302,
         0.3451,
         0.302,
         0.6538,
         673670.0,
         1557200.0,
         356715.0,
         2984434.0,
         96.68
        ],
        [
         "8baa6519a6704f0fb4b7e1e6ebace182",
         "5",
         "20",
         "400",
         "5",
         "sqrt",
         "1",
         0.6692,
         0.6597,
         0.6266,
         0.6808,
         1380275.0,
         822500.0,
         647204.0,
         1206869.0,
         0.529,
         0.3268,
         0.2871,
         0.6701,
         690491.0,
         1714199.0,
         339894.0,
         2827435.0,
         105.368
        ],
        [
         "694086d794af4bca8b38e03438939f09",
         "8",
         "20",
         "400",
         "5",
         "sqrt",
         "1",
         0.6591,
         0.6768,
         0.6454,
         0.6626,
         1343504.0,
         738036.0,
         683975.0,
         1291333.0,
         0.5286,
         0.345,
         0.3039,
         0.6485,
         668170.0,
         1530316.0,
         362215.0,
         3011318.0,
         95.643
        ],
        [
         "eb5bea983406403c8503168c98f84a75",
         "8",
         "20",
         "400",
         "5",
         "sqrt",
         "2",
         0.6649,
         0.6888,
         0.6538,
         0.6677,
         1330897.0,
         704608.0,
         662391.0,
         1290620.0,
         0.5251,
         0.3403,
         0.3069,
         0.6386,
         860561.0,
         1943758.0,
         486977.0,
         3794869.0,
         108.11
        ],
        [
         "2b777789129641608738b551b0643539",
         "8",
         "20",
         "400",
         "20",
         "sqrt",
         "2",
         0.665,
         0.6885,
         0.6528,
         0.6681,
         1331759.0,
         708431.0,
         661529.0,
         1286797.0,
         0.5248,
         0.3409,
         0.3059,
         0.6392,
         861304.0,
         1954681.0,
         486234.0,
         3783946.0,
         108.52
        ],
        [
         "0ec23fff330b4203ad99bb7ce378f339",
         "8",
         "20",
         "400",
         "10",
         "sqrt",
         "2",
         0.6638,
         0.6892,
         0.6534,
         0.6664,
         1328340.0,
         704666.0,
         664948.0,
         1290562.0,
         0.5242,
         0.3417,
         0.3068,
         0.637,
         858416.0,
         1939274.0,
         489122.0,
         3799353.0,
         107.774
        ],
        [
         "ba29b9e91c2946038c382c777a250f7b",
         "5",
         "20",
         "400",
         "5",
         "sqrt",
         "2",
         0.6506,
         0.674,
         0.6463,
         0.6517,
         1298942.0,
         710894.0,
         694346.0,
         1284334.0,
         0.5152,
         0.3273,
         0.2999,
         0.6279,
         846179.0,
         1975042.0,
         501359.0,
         3763585.0,
         97.612
        ],
        [
         "ee55c6f9015047fda83e3db9a7383e8c",
         "5",
         "20",
         "400",
         "20",
         "sqrt",
         "1",
         0.6322,
         0.6575,
         0.6371,
         0.631,
         1279356.0,
         728838.0,
         748123.0,
         1300531.0,
         0.5079,
         0.3237,
         0.2971,
         0.6174,
         636148.0,
         1504748.0,
         394237.0,
         3036886.0,
         78.205
        ],
        [
         "e05ca267dbd34db290072953056e027f",
         "5",
         "20",
         "400",
         "10",
         "sqrt",
         "1",
         0.6322,
         0.6575,
         0.6371,
         0.631,
         1279356.0,
         728838.0,
         748123.0,
         1300531.0,
         0.5079,
         0.3237,
         0.2971,
         0.6174,
         636148.0,
         1504748.0,
         394237.0,
         3036886.0,
         76.735
        ],
        [
         "4c7339f5a0b840efaf23e371862f1df9",
         "5",
         "20",
         "400",
         "5",
         "sqrt",
         "1",
         0.6322,
         0.6575,
         0.6371,
         0.631,
         1279356.0,
         728838.0,
         748123.0,
         1300531.0,
         0.5079,
         0.3237,
         0.2971,
         0.6174,
         636148.0,
         1504748.0,
         394237.0,
         3036886.0,
         78.592
        ],
        [
         "9434b2ae9a05476ab1497e8da8ebbb9f",
         "5",
         "20",
         "400",
         "20",
         "sqrt",
         "2",
         0.6322,
         0.6702,
         0.6454,
         0.629,
         1253843.0,
         688981.0,
         739445.0,
         1306247.0,
         0.5048,
         0.3248,
         0.3009,
         0.6078,
         819037.0,
         1903093.0,
         528501.0,
         3835534.0,
         84.899
        ],
        [
         "2f5f43c7d39b4bb796eccd6ec81f4550",
         "5",
         "20",
         "400",
         "10",
         "sqrt",
         "2",
         0.6322,
         0.6702,
         0.6454,
         0.629,
         1253843.0,
         688981.0,
         739445.0,
         1306247.0,
         0.5048,
         0.3248,
         0.3009,
         0.6078,
         819037.0,
         1903093.0,
         528501.0,
         3835534.0,
         85.767
        ],
        [
         "247a1d327011459dba90b4d8da9cc2e3",
         "5",
         "20",
         "400",
         "5",
         "sqrt",
         "2",
         0.6322,
         0.6702,
         0.6454,
         0.629,
         1253843.0,
         688981.0,
         739445.0,
         1306247.0,
         0.5048,
         0.3248,
         0.3009,
         0.6078,
         819037.0,
         1903093.0,
         528501.0,
         3835534.0,
         87.589
        ],
        [
         "e70461b151284087a2fca0f923054ec8",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         107.05
        ],
        [
         "24b9141d488941318e768b5615687abb",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         15.568
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "run_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "maxDepth",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "numTrees",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "maxBins",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "minInstancesPerNode",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "featureSubsetStrategy",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "window",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "train_f2_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "train_pr_auc",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "train_precision",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "train_recall",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "train_true_positives",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "train_false_positives",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "train_false_negatives",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "train_true_negatives",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "val_f2_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "val_pr_auc",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "val_precision",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "val_recall",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "val_true_positives",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "val_false_positives",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "val_false_negatives",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "val_true_negatives",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "wall_time_sec",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment_id_mlflow = mlflow.get_experiment_by_name(\"/Workspace/Shared/Team_2_1/rf/random_forest_grid_search\").experiment_id\n",
    "\n",
    "runs = client.search_runs(\n",
    "    experiment_id_mlflow, \n",
    "    filter_string=\"attribute.start_time >= 1764974781390\",\n",
    "    order_by=[\"metrics.val_f2_score DESC\"]\n",
    ")\n",
    "\n",
    "results = []\n",
    "for run in runs:\n",
    "    start_time = run.info.start_time\n",
    "    end_time = run.info.end_time\n",
    "    wall_time_sec = None\n",
    "    if start_time and end_time:\n",
    "        wall_time_sec = (end_time - start_time) / 1000.0  # ms to seconds\n",
    "    results.append({\n",
    "        \"run_id\": run.info.run_id,\n",
    "        \"maxDepth\": run.data.params.get(\"maxDepth\"),\n",
    "        \"numTrees\": run.data.params.get(\"numTrees\"),\n",
    "        \"maxBins\": run.data.params.get(\"maxBins\"),\n",
    "        \"minInstancesPerNode\": run.data.params.get(\"minInstancesPerNode\"),\n",
    "        \"featureSubsetStrategy\": run.data.params.get(\"featureSubsetStrategy\"),\n",
    "        \"window\": run.data.params.get(\"window\"),\n",
    "        \"train_f2_score\": run.data.metrics.get(\"train_f2_score\"),\n",
    "        \"train_pr_auc\": run.data.metrics.get(\"train_pr_auc\"),\n",
    "        \"train_precision\": run.data.metrics.get(\"train_precision\"),\n",
    "        \"train_recall\": run.data.metrics.get(\"train_recall\"),\n",
    "        \"train_true_positives\": run.data.metrics.get(\"train_true_positives\"),\n",
    "        \"train_false_positives\": run.data.metrics.get(\"train_false_positives\"),\n",
    "        \"train_false_negatives\": run.data.metrics.get(\"train_false_negatives\"),\n",
    "        \"train_true_negatives\": run.data.metrics.get(\"train_true_negatives\"),\n",
    "        \"val_f2_score\": run.data.metrics.get(\"val_f2_score\"),\n",
    "        \"val_pr_auc\": run.data.metrics.get(\"val_pr_auc\"),\n",
    "        \"val_precision\": run.data.metrics.get(\"val_precision\"),\n",
    "        \"val_recall\": run.data.metrics.get(\"val_recall\"),\n",
    "        \"val_true_positives\": run.data.metrics.get(\"val_true_positives\"),\n",
    "        \"val_false_positives\": run.data.metrics.get(\"val_false_positives\"),\n",
    "        \"val_false_negatives\": run.data.metrics.get(\"val_false_negatives\"),\n",
    "        \"val_true_negatives\": run.data.metrics.get(\"val_true_negatives\"),\n",
    "        \"wall_time_sec\": wall_time_sec\n",
    "    })\n",
    "\n",
    "results_df = spark.createDataFrame(pd.DataFrame(results))\n",
    "print(f\"Total runs exported: {results_df.count()}\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "967f3113-cee8-45d0-91ea-e23722005ed0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Features Importance"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features by importance:\nactual_to_crs_time_to_next_flight_diff_mins_clean: 0.2836\ncrs_time_to_next_flight_diff_mins: 0.1688\nCRS_ARR_TIME_BLOCK_idx: 0.0804\nCRS_DEP_TIME_BLOCK_idx: 0.0568\nDEST_idx: 0.0346\nMONTH: 0.0310\nORIGIN_idx: 0.0272\nprev_flight_arr_delay_clean: 0.0233\nOP_UNIQUE_CARRIER_idx: 0.0230\nHourlyAltimeterSetting: 0.0178\norigin_pagerank: 0.0164\nDEST_LON: 0.0140\nlowest_cloud_ft: 0.0130\nORIGIN_LONG: 0.0130\norigin_out_degree: 0.0126\nDAY_OF_MONTH: 0.0121\nDEST_STATE_ABR_idx: 0.0119\nORIGIN_STATE_ABR_idx: 0.0116\nDISTANCE: 0.0114\nCRS_ELAPSED_TIME: 0.0114\nDEST_ELEVATION_FT: 0.0113\ndest_in_degree: 0.0104\ndest_pagerank: 0.0097\nsnow: 0.0092\nORIGIN_LAT: 0.0090\nhas_ovc: 0.0082\nORIGIN_ELEVATION_FT: 0.0076\nDEST_LAT: 0.0075\noverall_cloud_frac_0_1: 0.0072\nQUARTER: 0.0065\nHourlyWindCardinalDirection_idx: 0.0062\nhighest_cloud_ft: 0.0059\nDAY_OF_WEEK: 0.0056\nHourlyWindSpeed: 0.0046\nIS_US_HOLIDAY: 0.0043\nreduced_visibility: 0.0031\nlight: 0.0022\nrain_or_drizzle: 0.0019\nHourlyWindGustSpeed: 0.0016\nORIGIN_SIZE_idx: 0.0008\nhas_bkn: 0.0008\nhas_few: 0.0007\nhas_sct: 0.0006\nfreezing_conditions: 0.0005\nDEST_SIZE_idx: 0.0005\nthunderstorm: 0.0003\nheavy: 0.0001\nspatial_effects: 0.0000\nhail_or_ice: 0.0000\nunknown_precip: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "runs = client.search_runs(experiment_id_mlflow, order_by=[\"metrics.f2_score DESC\"])\n",
    "\n",
    "best_run_id = runs[0].info.run_id\n",
    "\n",
    "# Load feature importance from the best run's model\n",
    "model_uri = f\"runs:/{best_run_id}/rf_model\"\n",
    "rf_model = mlflow.spark.load_model(model_uri)\n",
    "\n",
    "# Extract feature importances and feature names\n",
    "importances = rf_model.stages[-1].featureImportances\n",
    "feature_names = rf_model.stages[0].getInputCols()\n",
    "\n",
    "# Pair and sort features by importance\n",
    "feature_importance = sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top features by importance:\")\n",
    "for name, score in feature_importance:\n",
    "    print(f\"{name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d60b5dc-e8c4-467b-98d6-a9693c3046cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Blind Test Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab799a3-92e1-494e-849b-e92a34a95bc6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Blind Test Run"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTotal training rows: 8,811,266\nTraining model on downsampled data...\nTest samples: 7,271,711\nRunning predictions...\n\nBLIND TEST RESULTS:\n================================================================================\nF2 Score:  0.5453\nPR-AUC:    0.3662\nPrecision: 0.3102\nRecall:    0.6728\n\nConfusion Matrix:\n  TP=933,786, FP=2,076,416\n  FN=454,118, TN=3,807,391\n================================================================================\n"
     ]
    }
   ],
   "source": [
    "best_parameters = {\n",
    "    'maxDepth': 10, \n",
    "    'numTrees': 20, \n",
    "    'maxBins': 400, \n",
    "    'minInstancesPerNode': 5, \n",
    "    'featureSubsetStrategy': 'sqrt'\n",
    "}\n",
    "\n",
    "columns_to_keep = list(set(numeric_cols_to_use + cat_cols_to_use + ['ARR_DEL15']))\n",
    "train_full_df = df_train_master.select(columns_to_keep)\n",
    "print(f\"\\nTotal training rows: {train_full_df.count():,}\")\n",
    "\n",
    "pipeline = get_rf_model(best_parameters, cat_cols_to_use, numeric_cols_to_use, use_weights=False)\n",
    "\n",
    "print(f\"Training model on downsampled data...\")\n",
    "model = pipeline.fit(train_full_df)\n",
    "\n",
    "test_df = spark.read.parquet(blind_test_set_path).select(columns_to_keep)\n",
    "print(f\"Test samples: {test_df.count():,}\")\n",
    "print(f\"Running predictions...\")\n",
    "\n",
    "test_pred = model.transform(test_df)\n",
    "\n",
    "f2, pr_auc, precision, recall, tp, fp, fn, tn = cv_eval(test_pred)\n",
    "\n",
    "print(\"\\nBLIND TEST RESULTS:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"F2 Score:  {f2:.4f}\")\n",
    "print(f\"PR-AUC:    {pr_auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TP={tp:,}, FP={fp:,}\")\n",
    "print(f\"  FN={fn:,}, TN={tn:,}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "068c40fe-df6c-4bb9-8db6-a0e3391755b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Full_unscaled_features model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96f2d7a4-c3a8-4431-a3b3-a70ad9f4cab1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Windows Training on Full Unscaled Features"
    }
   },
   "outputs": [],
   "source": [
    "def get_rf_model_with_prebuilt_features(params, feature_col='full_unscaled_features', use_weights=False):\n",
    "    from pyspark.ml.classification import RandomForestClassifier\n",
    "    from pyspark.ml import Pipeline\n",
    "    \n",
    "    rf_params = {\n",
    "        'labelCol': \"ARR_DEL15\",\n",
    "        'featuresCol': feature_col,\n",
    "        'maxDepth': params['maxDepth'],\n",
    "        'numTrees': params['numTrees'],\n",
    "        'maxBins': params.get('maxBins', 400),\n",
    "        'minInstancesPerNode': params.get('minInstancesPerNode', 1),\n",
    "        'featureSubsetStrategy': params.get('featureSubsetStrategy', 'auto'),\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    if use_weights:\n",
    "        rf_params['weightCol'] = 'weight'\n",
    "    \n",
    "    rf = RandomForestClassifier(**rf_params)\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def model_eval_on_folds_prebuilt(folds_input_path, param_grid, feature_col='full_unscaled_features',\n",
    "                                  N_SPLITS=N_SPLITS, use_weights=False):\n",
    "    from pyspark.sql import functions as F\n",
    "    from itertools import product\n",
    "    import numpy as np\n",
    "    \n",
    "    parameter_names = list(param_grid.keys())\n",
    "    parameter_values = list(param_grid.values())\n",
    "    parameters = list(product(*parameter_values))\n",
    "    \n",
    "    best_score = 0\n",
    "    best_parameters = None\n",
    "    best_fold_predictions = []\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"\\nEvaluating Random Forest Model with Pre-built Features\")\n",
    "    print(f\"Feature column: {feature_col}\")\n",
    "    print(f\"Number of windows: {N_SPLITS}\")\n",
    "    print(f\"Parameter combinations to test: {len(parameters)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for p in parameters:\n",
    "        param_print = {x[0]:x[1] for x in zip(parameter_names, p)}\n",
    "        print(f\"\\nTesting Parameters: {param_print}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        scores_f2 = []\n",
    "        scores_pr = []\n",
    "        scores_precision = []\n",
    "        scores_recall = []\n",
    "        confusion_list = []\n",
    "        fold_predictions = []\n",
    "        \n",
    "        for i in range(N_SPLITS):\n",
    "            print(f\"  Window {i+1}:\")\n",
    "            train_path = f\"{folds_input_path}/window_{i+1}_train\"\n",
    "            val_path = f\"{folds_input_path}/window_{i+1}_val\"\n",
    "            \n",
    "            train_df = spark.read.parquet(train_path)\n",
    "            val_df = spark.read.parquet(val_path)\n",
    "            \n",
    "            print(f\"    Train path: {train_path}\")\n",
    "            print(f\"    Val path: {val_path}\")\n",
    "            print(f\"    Train samples: {train_df.count():,}\")\n",
    "            print(f\"    Val samples: {val_df.count():,}\")\n",
    "            \n",
    "            pipeline = get_rf_model_with_prebuilt_features(param_print, feature_col, use_weights)\n",
    "            model = pipeline.fit(train_df)\n",
    "            preds = model.transform(val_df)\n",
    "            \n",
    "            f2, pr_auc, precision, recall, tp, fp, fn, tn = cv_eval(preds)\n",
    "            scores_f2.append(f2)\n",
    "            scores_pr.append(pr_auc)\n",
    "            scores_precision.append(precision)\n",
    "            scores_recall.append(recall)\n",
    "            confusion_list.append({\"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn})\n",
    "            fold_predictions.append((i+1, preds, f2, pr_auc, precision, recall, tp, fp, fn, tn))\n",
    "            \n",
    "            print(f\"    F2: {f2:.4f}, PR-AUC: {pr_auc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "            print(f\"    TP={tp}, FP={fp}, FN={fn}, TN={tn}\")\n",
    "        \n",
    "        avg_f2 = np.mean(scores_f2)\n",
    "        avg_pr = np.mean(scores_pr)\n",
    "        avg_precision = np.mean(scores_precision)\n",
    "        avg_recall = np.mean(scores_recall)\n",
    "        \n",
    "        print(f\"\\n  Average across {N_SPLITS} windows:\")\n",
    "        print(f\"    F2: {avg_f2:.4f} (±{np.std(scores_f2):.4f})\")\n",
    "        print(f\"    PR-AUC: {avg_pr:.4f} (±{np.std(scores_pr):.4f})\")\n",
    "        print(f\"    Precision: {avg_precision:.4f} (±{np.std(scores_precision):.4f})\")\n",
    "        print(f\"    Recall: {avg_recall:.4f} (±{np.std(scores_recall):.4f})\")\n",
    "        \n",
    "        if avg_f2 > best_score:\n",
    "            best_score = avg_f2\n",
    "            best_parameters = param_print\n",
    "            best_fold_predictions = fold_predictions\n",
    "            print(f\"  ✓ NEW BEST F2: {best_score:.4f}\")\n",
    "        \n",
    "        all_results.append({\n",
    "            'parameters': param_print,\n",
    "            'f2_scores': scores_f2,\n",
    "            'pr_auc_scores': scores_pr,\n",
    "            'precision_scores': scores_precision,\n",
    "            'recall_scores': scores_recall,\n",
    "            'confusion': confusion_list,\n",
    "            'f2_mean': float(avg_f2),\n",
    "            'f2_std': float(np.std(scores_f2)),\n",
    "            'pr_auc_mean': float(avg_pr),\n",
    "            'pr_auc_std': float(np.std(scores_pr)),\n",
    "            'precision_mean': float(avg_precision),\n",
    "            'precision_std': float(np.std(scores_precision)),\n",
    "            'recall_mean': float(avg_recall),\n",
    "            'recall_std': float(np.std(scores_recall))\n",
    "        })\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    best_result = [r for r in all_results if r['f2_mean'] == best_score][0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MODEL EVALUATION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Feature column used: {feature_col}\")\n",
    "    print(f\"Best Parameters: {best_parameters}\")\n",
    "    print(f\"Best F2: {best_result['f2_mean']:.4f} (±{best_result['f2_std']:.4f})\")\n",
    "    print(f\"Best PR-AUC: {best_result['pr_auc_mean']:.4f} (±{best_result['pr_auc_std']:.4f})\")\n",
    "    print(f\"Best Precision: {best_result['precision_mean']:.4f} (±{best_result['precision_std']:.4f})\")\n",
    "    print(f\"Best Recall: {best_result['recall_mean']:.4f} (±{best_result['recall_std']:.4f})\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        'feature_col': feature_col,\n",
    "        'best_parameters': best_parameters,\n",
    "        'best_f2_mean': best_result['f2_mean'],\n",
    "        'best_f2_std': best_result['f2_std'],\n",
    "        'best_pr_auc_mean': best_result['pr_auc_mean'],\n",
    "        'best_pr_auc_std': best_result['pr_auc_std'],\n",
    "        'best_precision_mean': best_result['precision_mean'],\n",
    "        'best_precision_std': best_result['precision_std'],\n",
    "        'best_recall_mean': best_result['recall_mean'],\n",
    "        'best_recall_std': best_result['recall_std'],\n",
    "        'best_f2_scores': best_result['f2_scores'],\n",
    "        'best_pr_auc_scores': best_result['pr_auc_scores'],\n",
    "        'best_precision_scores': best_result['precision_scores'],\n",
    "        'best_recall_scores': best_result['recall_scores'],\n",
    "        'best_confusion': best_result['confusion'],\n",
    "        'all_results': all_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4534226a-39c6-4ca0-906a-504c5aff2c20",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train on Full_Unscaled_Features Only"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations: 1\nTotal runs (combinations × windows): 2\n\nTesting: {'maxDepth': 10, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 10, 'featureSubsetStrategy': 'sqrt'}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 21:02:38 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmpey6biqey, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 1: F2=0.5294, Recall=0.6659\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/07 21:04:46 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /local_disk0/repl_tmp_data/ReplId-19afa-4ed56-6/tmpnewvm2v0, flavor: spark). Fall back to return ['pyspark==4.0.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Window 2: F2=0.5240, Recall=0.6550\n  Average: F2=0.5267 (±0.0027), Recall=0.6604\n  ✓ NEW BEST!\n\n================================================================================\nGRID SEARCH COMPLETE - full_unscaled_features\n================================================================================\nBest Parameters: {'maxDepth': 10, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 10, 'featureSubsetStrategy': 'sqrt'}\nBest F2: 0.5267\n================================================================================\n\nResults stored in: rf_results_full_features\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MLFLOW GRID SEARCH - PREBUILT FEATURES (full_unscaled_features)\n",
    "# ============================================================\n",
    "\n",
    "experiment_name_mlflow = \"/Workspace/Shared/Team_2_1/rf/random_forest_full_unscaled_features\"\n",
    "\n",
    "try:\n",
    "    experiment_id_mlflow = mlflow.create_experiment(\n",
    "        name=experiment_name_mlflow,\n",
    "        artifact_location=\"dbfs:/student-groups/Group_02_01/experiments/rf\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    experiment_mlflow = mlflow.get_experiment_by_name(experiment_name_mlflow)\n",
    "    if experiment_mlflow:\n",
    "        experiment_id_mlflow = experiment_mlflow.experiment_id\n",
    "\n",
    "mlflow.set_experiment(experiment_name_mlflow)\n",
    "\n",
    "# Use your param_grid\n",
    "param_grid_mlflow = param_grid  # Or define it explicitly if needed\n",
    "\n",
    "parameter_names_mlflow = list(param_grid_mlflow.keys())\n",
    "parameter_values_mlflow = list(param_grid_mlflow.values())\n",
    "parameters_mlflow = list(product(*parameter_values_mlflow))\n",
    "\n",
    "print(f\"Total parameter combinations: {len(parameters_mlflow)}\")\n",
    "print(f\"Total runs (combinations × windows): {len(parameters_mlflow) * N_SPLITS}\")\n",
    "\n",
    "best_f2_mlflow = 0\n",
    "best_params_mlflow = None\n",
    "all_results_mlflow = []\n",
    "\n",
    "for p in parameters_mlflow:\n",
    "    param_dict_mlflow = {x[0]:x[1] for x in zip(parameter_names_mlflow, p)}\n",
    "    print(f\"\\nTesting: {param_dict_mlflow}\")\n",
    "    \n",
    "    f2_scores_mlflow = []\n",
    "    pr_scores_mlflow = []\n",
    "    precision_scores_mlflow = []\n",
    "    recall_scores_mlflow = []\n",
    "    \n",
    "    for i in range(N_SPLITS):\n",
    "        train_path_mlflow = f\"{folds_input_path}/window_{i+1}_train\"\n",
    "        val_path_mlflow = f\"{folds_input_path}/window_{i+1}_val\"\n",
    "        \n",
    "        # Select only the prebuilt feature column and target\n",
    "        columns_to_keep = ['full_unscaled_features', 'ARR_DEL15']\n",
    "        train_df_mlflow = spark.read.parquet(train_path_mlflow).select(columns_to_keep)\n",
    "        val_df_mlflow = spark.read.parquet(val_path_mlflow).select(columns_to_keep)\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"d{param_dict_mlflow['maxDepth']}_t{param_dict_mlflow['numTrees']}_w{i+1}\"):\n",
    "            # Use prebuilt features pipeline\n",
    "            pipeline_mlflow = get_rf_model_with_prebuilt_features(\n",
    "                param_dict_mlflow, \n",
    "                feature_col='full_unscaled_features', \n",
    "                use_weights=False\n",
    "            )\n",
    "            \n",
    "            model_mlflow = pipeline_mlflow.fit(train_df_mlflow)\n",
    "            preds_mlflow = model_mlflow.transform(val_df_mlflow)\n",
    "            \n",
    "            f2_mlflow, pr_auc_mlflow, precision_mlflow, recall_mlflow, tp_mlflow, fp_mlflow, fn_mlflow, tn_mlflow = cv_eval(preds_mlflow)\n",
    "            \n",
    "            f2_scores_mlflow.append(f2_mlflow)\n",
    "            pr_scores_mlflow.append(pr_auc_mlflow)\n",
    "            precision_scores_mlflow.append(precision_mlflow)\n",
    "            recall_scores_mlflow.append(recall_mlflow)\n",
    "            \n",
    "            # Log parameters\n",
    "            mlflow.log_param(\"maxDepth\", param_dict_mlflow['maxDepth'])\n",
    "            mlflow.log_param(\"numTrees\", param_dict_mlflow['numTrees'])\n",
    "            mlflow.log_param(\"maxBins\", param_dict_mlflow['maxBins'])\n",
    "            mlflow.log_param(\"minInstancesPerNode\", param_dict_mlflow['minInstancesPerNode'])\n",
    "            mlflow.log_param(\"featureSubsetStrategy\", param_dict_mlflow['featureSubsetStrategy'])\n",
    "            mlflow.log_param(\"feature_col\", \"full_unscaled_features\")\n",
    "            mlflow.log_param(\"window\", i+1)\n",
    "            \n",
    "            # Log metrics\n",
    "            mlflow.log_metric(\"f2_score\", f2_mlflow)\n",
    "            mlflow.log_metric(\"pr_auc\", pr_auc_mlflow)\n",
    "            mlflow.log_metric(\"precision\", precision_mlflow)\n",
    "            mlflow.log_metric(\"recall\", recall_mlflow)\n",
    "            mlflow.log_metric(\"true_positives\", tp_mlflow)\n",
    "            mlflow.log_metric(\"false_positives\", fp_mlflow)\n",
    "            mlflow.log_metric(\"false_negatives\", fn_mlflow)\n",
    "            mlflow.log_metric(\"true_negatives\", tn_mlflow)\n",
    "            \n",
    "            # Log model\n",
    "            mlflow.spark.log_model(model_mlflow, artifact_path=\"rf_model\")\n",
    "            \n",
    "            print(f\"  Window {i+1}: F2={f2_mlflow:.4f}, Recall={recall_mlflow:.4f}\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_f2_mlflow = np.mean(f2_scores_mlflow)\n",
    "    avg_pr_mlflow = np.mean(pr_scores_mlflow)\n",
    "    avg_precision_mlflow = np.mean(precision_scores_mlflow)\n",
    "    avg_recall_mlflow = np.mean(recall_scores_mlflow)\n",
    "    \n",
    "    print(f\"  Average: F2={avg_f2_mlflow:.4f} (±{np.std(f2_scores_mlflow):.4f}), Recall={avg_recall_mlflow:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    all_results_mlflow.append({\n",
    "        'parameters': param_dict_mlflow,\n",
    "        'avg_f2': avg_f2_mlflow,\n",
    "        'std_f2': np.std(f2_scores_mlflow),\n",
    "        'avg_pr_auc': avg_pr_mlflow,\n",
    "        'avg_precision': avg_precision_mlflow,\n",
    "        'avg_recall': avg_recall_mlflow,\n",
    "        'fold_f2_scores': f2_scores_mlflow\n",
    "    })\n",
    "    \n",
    "    # Track best parameters\n",
    "    if avg_f2_mlflow > best_f2_mlflow:\n",
    "        best_f2_mlflow = avg_f2_mlflow\n",
    "        best_params_mlflow = param_dict_mlflow\n",
    "        print(f\"  ✓ NEW BEST!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GRID SEARCH COMPLETE - full_unscaled_features\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best Parameters: {best_params_mlflow}\")\n",
    "print(f\"Best F2: {best_f2_mlflow:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create results dictionary matching your original format\n",
    "rf_results_full_features = {\n",
    "    'best_parameters': best_params_mlflow,\n",
    "    'best_f2': best_f2_mlflow,\n",
    "    'all_results': all_results_mlflow\n",
    "}\n",
    "\n",
    "print(f\"\\nResults stored in: rf_results_full_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee245ce-60b6-4f7d-a1f1-b3f904e7aec2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Blind Test on Full_Unscaled_Features only"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'maxDepth': 10, 'numTrees': 20, 'maxBins': 400, 'minInstancesPerNode': 10, 'featureSubsetStrategy': 'sqrt'}\nFeature column: full_unscaled_features\n\nTotal training rows: 8,812,266\nClass distribution in training data:\n+---------+-------+\n|ARR_DEL15|  count|\n+---------+-------+\n|        1|4405402|\n|        0|4406864|\n+---------+-------+\n\nTraining model on downsampled data...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaModelWrapper.__del__ at 0x7f04285827a0>\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/mllib/common.py\", line 152, in __del__\n    assert self._sc._gateway is not None\n           ^^^^^^^^\nAttributeError: 'BinaryClassificationMetrics' object has no attribute '_sc'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8516020384389433>, line 27\u001B[0m\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# =======================\u001B[39;00m\n",
       "\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# TRAIN MODEL\u001B[39;00m\n",
       "\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# =======================\u001B[39;00m\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining model on downsampled data...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 27\u001B[0m model_full_unscaled_features \u001B[38;5;241m=\u001B[39m pipeline_full_unscaled_features\u001B[38;5;241m.\u001B[39mfit(train_full_df)\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# =======================\u001B[39;00m\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# TEST ON BLIND TEST SET\u001B[39;00m\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# =======================\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m test_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(blind_test_set_path)\u001B[38;5;241m.\u001B[39mselect(columns_to_keep_full_unscaled_only)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:483\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    479\u001B[0m call_original \u001B[38;5;241m=\u001B[39m update_wrapper_extended(call_original, original)\n",
       "\u001B[1;32m    481\u001B[0m event_logger\u001B[38;5;241m.\u001B[39mlog_patch_function_start(args, kwargs)\n",
       "\u001B[0;32m--> 483\u001B[0m patch_function(call_original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    485\u001B[0m session\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msucceeded\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    486\u001B[0m event_logger\u001B[38;5;241m.\u001B[39mlog_patch_function_success(args, kwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:182\u001B[0m, in \u001B[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001B[0;34m(original, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    179\u001B[0m     managed_run \u001B[38;5;241m=\u001B[39m create_managed_run()\n",
       "\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 182\u001B[0m     result \u001B[38;5;241m=\u001B[39m patch_function(original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mException\u001B[39;00m, \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m):\n",
       "\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001B[39;00m\n",
       "\u001B[1;32m    185\u001B[0m     \u001B[38;5;66;03m# that runs are terminated if a user prematurely interrupts training execution\u001B[39;00m\n",
       "\u001B[1;32m    186\u001B[0m     \u001B[38;5;66;03m# (e.g. via sigint / ctrl-c)\u001B[39;00m\n",
       "\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m managed_run:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/pyspark/ml/__init__.py:1172\u001B[0m, in \u001B[0;36mautolog.<locals>.patched_fit\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mshould_log():\n",
       "\u001B[1;32m   1171\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _AUTOLOGGING_METRICS_MANAGER\u001B[38;5;241m.\u001B[39mdisable_log_post_training_metrics():\n",
       "\u001B[0;32m-> 1172\u001B[0m         fit_result \u001B[38;5;241m=\u001B[39m fit_mlflow(original, \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1173\u001B[0m     \u001B[38;5;66;03m# In some cases the `fit_result` may be an iterator of spark models.\u001B[39;00m\n",
       "\u001B[1;32m   1174\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m should_log_post_training_metrics \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fit_result, Model):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/pyspark/ml/__init__.py:1158\u001B[0m, in \u001B[0;36mautolog.<locals>.fit_mlflow\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1156\u001B[0m input_training_df \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mpersist(StorageLevel\u001B[38;5;241m.\u001B[39mMEMORY_AND_DISK)\n",
       "\u001B[1;32m   1157\u001B[0m _log_pretraining_metadata(estimator, params, input_training_df)\n",
       "\u001B[0;32m-> 1158\u001B[0m spark_model \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   1159\u001B[0m _log_posttraining_metadata(estimator, spark_model, params, input_training_df)\n",
       "\u001B[1;32m   1160\u001B[0m input_training_df\u001B[38;5;241m.\u001B[39munpersist()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:474\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    471\u001B[0m         original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n",
       "\u001B[1;32m    472\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n",
       "\u001B[0;32m--> 474\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:425\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001B[0;34m(original_fn, og_args, og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    423\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_start(og_args, og_kwargs)\n",
       "\u001B[0;32m--> 425\u001B[0m     original_fn_result \u001B[38;5;241m=\u001B[39m original_fn(\u001B[38;5;241m*\u001B[39mog_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mog_kwargs)\n",
       "\u001B[1;32m    427\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_success(og_args, og_kwargs)\n",
       "\u001B[1;32m    428\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_fn_result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:471\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001B[0;34m(*_og_args, **_og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001B[39;00m\n",
       "\u001B[1;32m    464\u001B[0m \u001B[38;5;66;03m# during original function execution, even if silent mode is enabled\u001B[39;00m\n",
       "\u001B[1;32m    465\u001B[0m \u001B[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001B[39;00m\n",
       "\u001B[1;32m    466\u001B[0m \u001B[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001B[39;00m\n",
       "\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n",
       "\u001B[1;32m    468\u001B[0m     disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    469\u001B[0m     reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    470\u001B[0m ):\n",
       "\u001B[0;32m--> 471\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n",
       "\u001B[1;32m    472\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    208\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py:136\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    134\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mtransform(dataset)\n",
       "\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n",
       "\u001B[0;32m--> 136\u001B[0m     model \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mfit(dataset)\n",
       "\u001B[1;32m    137\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(model)\n",
       "\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m indexOfLastEstimator:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:483\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    479\u001B[0m call_original \u001B[38;5;241m=\u001B[39m update_wrapper_extended(call_original, original)\n",
       "\u001B[1;32m    481\u001B[0m event_logger\u001B[38;5;241m.\u001B[39mlog_patch_function_start(args, kwargs)\n",
       "\u001B[0;32m--> 483\u001B[0m patch_function(call_original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    485\u001B[0m session\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msucceeded\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    486\u001B[0m event_logger\u001B[38;5;241m.\u001B[39mlog_patch_function_success(args, kwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:182\u001B[0m, in \u001B[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001B[0;34m(original, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    179\u001B[0m     managed_run \u001B[38;5;241m=\u001B[39m create_managed_run()\n",
       "\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 182\u001B[0m     result \u001B[38;5;241m=\u001B[39m patch_function(original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mException\u001B[39;00m, \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m):\n",
       "\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001B[39;00m\n",
       "\u001B[1;32m    185\u001B[0m     \u001B[38;5;66;03m# that runs are terminated if a user prematurely interrupts training execution\u001B[39;00m\n",
       "\u001B[1;32m    186\u001B[0m     \u001B[38;5;66;03m# (e.g. via sigint / ctrl-c)\u001B[39;00m\n",
       "\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m managed_run:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/pyspark/ml/__init__.py:1180\u001B[0m, in \u001B[0;36mautolog.<locals>.patched_fit\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_result\n",
       "\u001B[1;32m   1179\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1180\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:474\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    471\u001B[0m         original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n",
       "\u001B[1;32m    472\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n",
       "\u001B[0;32m--> 474\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:425\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001B[0;34m(original_fn, og_args, og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    423\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_start(og_args, og_kwargs)\n",
       "\u001B[0;32m--> 425\u001B[0m     original_fn_result \u001B[38;5;241m=\u001B[39m original_fn(\u001B[38;5;241m*\u001B[39mog_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mog_kwargs)\n",
       "\u001B[1;32m    427\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_success(og_args, og_kwargs)\n",
       "\u001B[1;32m    428\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_fn_result\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:471\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001B[0;34m(*_og_args, **_og_kwargs)\u001B[0m\n",
       "\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001B[39;00m\n",
       "\u001B[1;32m    464\u001B[0m \u001B[38;5;66;03m# during original function execution, even if silent mode is enabled\u001B[39;00m\n",
       "\u001B[1;32m    465\u001B[0m \u001B[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001B[39;00m\n",
       "\u001B[1;32m    466\u001B[0m \u001B[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001B[39;00m\n",
       "\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n",
       "\u001B[1;32m    468\u001B[0m     disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    469\u001B[0m     reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    470\u001B[0m ):\n",
       "\u001B[0;32m--> 471\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n",
       "\u001B[1;32m    472\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
       "\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n",
       "\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n",
       "\u001B[1;32m    208\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:398\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n",
       "\u001B[0;32m--> 398\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_java(dataset)\n",
       "\u001B[1;32m    399\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n",
       "\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:395\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n",
       "\u001B[1;32m    392\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    394\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n",
       "\u001B[0;32m--> 395\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mfit(dataset\u001B[38;5;241m.\u001B[39m_jdf)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:275\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    271\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    273\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    274\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 275\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: [FIELD_NOT_FOUND] No such struct field `weight` in `full_unscaled_features`, `ARR_DEL15`. SQLSTATE: 42704"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "IllegalArgumentException",
        "evalue": "[FIELD_NOT_FOUND] No such struct field `weight` in `full_unscaled_features`, `ARR_DEL15`. SQLSTATE: 42704"
       },
       "metadata": {
        "errorSummary": "[FIELD_NOT_FOUND] No such struct field `weight` in `full_unscaled_features`, `ARR_DEL15`. SQLSTATE: 42704"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "FIELD_NOT_FOUND",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "42704",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)",
        "File \u001B[0;32m<command-8516020384389433>, line 27\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# =======================\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# TRAIN MODEL\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# =======================\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining model on downsampled data...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 27\u001B[0m model_full_unscaled_features \u001B[38;5;241m=\u001B[39m pipeline_full_unscaled_features\u001B[38;5;241m.\u001B[39mfit(train_full_df)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# =======================\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;66;03m# TEST ON BLIND TEST SET\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# =======================\u001B[39;00m\n\u001B[1;32m     32\u001B[0m test_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mparquet(blind_test_set_path)\u001B[38;5;241m.\u001B[39mselect(columns_to_keep_full_unscaled_only)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:483\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    479\u001B[0m call_original \u001B[38;5;241m=\u001B[39m update_wrapper_extended(call_original, original)\n\u001B[1;32m    481\u001B[0m event_logger\u001B[38;5;241m.\u001B[39mlog_patch_function_start(args, kwargs)\n\u001B[0;32m--> 483\u001B[0m patch_function(call_original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    485\u001B[0m session\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msucceeded\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    486\u001B[0m event_logger\u001B[38;5;241m.\u001B[39mlog_patch_function_success(args, kwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:182\u001B[0m, in \u001B[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001B[0;34m(original, *args, **kwargs)\u001B[0m\n\u001B[1;32m    179\u001B[0m     managed_run \u001B[38;5;241m=\u001B[39m create_managed_run()\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 182\u001B[0m     result \u001B[38;5;241m=\u001B[39m patch_function(original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mException\u001B[39;00m, \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m):\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001B[39;00m\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;66;03m# that runs are terminated if a user prematurely interrupts training execution\u001B[39;00m\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;66;03m# (e.g. via sigint / ctrl-c)\u001B[39;00m\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m managed_run:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/pyspark/ml/__init__.py:1172\u001B[0m, in \u001B[0;36mautolog.<locals>.patched_fit\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mshould_log():\n\u001B[1;32m   1171\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _AUTOLOGGING_METRICS_MANAGER\u001B[38;5;241m.\u001B[39mdisable_log_post_training_metrics():\n\u001B[0;32m-> 1172\u001B[0m         fit_result \u001B[38;5;241m=\u001B[39m fit_mlflow(original, \u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1173\u001B[0m     \u001B[38;5;66;03m# In some cases the `fit_result` may be an iterator of spark models.\u001B[39;00m\n\u001B[1;32m   1174\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m should_log_post_training_metrics \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(fit_result, Model):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/pyspark/ml/__init__.py:1158\u001B[0m, in \u001B[0;36mautolog.<locals>.fit_mlflow\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1156\u001B[0m input_training_df \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mpersist(StorageLevel\u001B[38;5;241m.\u001B[39mMEMORY_AND_DISK)\n\u001B[1;32m   1157\u001B[0m _log_pretraining_metadata(estimator, params, input_training_df)\n\u001B[0;32m-> 1158\u001B[0m spark_model \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1159\u001B[0m _log_posttraining_metadata(estimator, spark_model, params, input_training_df)\n\u001B[1;32m   1160\u001B[0m input_training_df\u001B[38;5;241m.\u001B[39munpersist()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:474\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n\u001B[1;32m    471\u001B[0m         original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n\u001B[1;32m    472\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n\u001B[0;32m--> 474\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:425\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001B[0;34m(original_fn, og_args, og_kwargs)\u001B[0m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    423\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_start(og_args, og_kwargs)\n\u001B[0;32m--> 425\u001B[0m     original_fn_result \u001B[38;5;241m=\u001B[39m original_fn(\u001B[38;5;241m*\u001B[39mog_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mog_kwargs)\n\u001B[1;32m    427\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_success(og_args, og_kwargs)\n\u001B[1;32m    428\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_fn_result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:471\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001B[0;34m(*_og_args, **_og_kwargs)\u001B[0m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001B[39;00m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;66;03m# during original function execution, even if silent mode is enabled\u001B[39;00m\n\u001B[1;32m    465\u001B[0m \u001B[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001B[39;00m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001B[39;00m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n\u001B[1;32m    468\u001B[0m     disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    469\u001B[0m     reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    470\u001B[0m ):\n\u001B[0;32m--> 471\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n\u001B[1;32m    472\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/pipeline.py:136\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    134\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mtransform(dataset)\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n\u001B[0;32m--> 136\u001B[0m     model \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mfit(dataset)\n\u001B[1;32m    137\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(model)\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m indexOfLastEstimator:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/MLWorkloadsInstrumentation/_pyspark.py:30\u001B[0m, in \u001B[0;36m_create_patch_function.<locals>.patched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 30\u001B[0m     result \u001B[38;5;241m=\u001B[39m original_method(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     31\u001B[0m     call_succeeded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:483\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    479\u001B[0m call_original \u001B[38;5;241m=\u001B[39m update_wrapper_extended(call_original, original)\n\u001B[1;32m    481\u001B[0m event_logger\u001B[38;5;241m.\u001B[39mlog_patch_function_start(args, kwargs)\n\u001B[0;32m--> 483\u001B[0m patch_function(call_original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    485\u001B[0m session\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msucceeded\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    486\u001B[0m event_logger\u001B[38;5;241m.\u001B[39mlog_patch_function_success(args, kwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:182\u001B[0m, in \u001B[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001B[0;34m(original, *args, **kwargs)\u001B[0m\n\u001B[1;32m    179\u001B[0m     managed_run \u001B[38;5;241m=\u001B[39m create_managed_run()\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 182\u001B[0m     result \u001B[38;5;241m=\u001B[39m patch_function(original, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    183\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mException\u001B[39;00m, \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m):\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;66;03m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001B[39;00m\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;66;03m# that runs are terminated if a user prematurely interrupts training execution\u001B[39;00m\n\u001B[1;32m    186\u001B[0m     \u001B[38;5;66;03m# (e.g. via sigint / ctrl-c)\u001B[39;00m\n\u001B[1;32m    187\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m managed_run:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/pyspark/ml/__init__.py:1180\u001B[0m, in \u001B[0;36mautolog.<locals>.patched_fit\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_result\n\u001B[1;32m   1179\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1180\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:474\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n\u001B[1;32m    471\u001B[0m         original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n\u001B[1;32m    472\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n\u001B[0;32m--> 474\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:425\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001B[0;34m(original_fn, og_args, og_kwargs)\u001B[0m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    423\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_start(og_args, og_kwargs)\n\u001B[0;32m--> 425\u001B[0m     original_fn_result \u001B[38;5;241m=\u001B[39m original_fn(\u001B[38;5;241m*\u001B[39mog_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mog_kwargs)\n\u001B[1;32m    427\u001B[0m     event_logger\u001B[38;5;241m.\u001B[39mlog_original_function_success(og_args, og_kwargs)\n\u001B[1;32m    428\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_fn_result\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/mlflow/utils/autologging_utils/safety.py:471\u001B[0m, in \u001B[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001B[0;34m(*_og_args, **_og_kwargs)\u001B[0m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001B[39;00m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;66;03m# during original function execution, even if silent mode is enabled\u001B[39;00m\n\u001B[1;32m    465\u001B[0m \u001B[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001B[39;00m\n\u001B[1;32m    466\u001B[0m \u001B[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001B[39;00m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n\u001B[1;32m    468\u001B[0m     disable_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    469\u001B[0m     reroute_warnings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    470\u001B[0m ):\n\u001B[0;32m--> 471\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m original(\u001B[38;5;241m*\u001B[39m_og_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m_og_kwargs)\n\u001B[1;32m    472\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m original_result\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    202\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    206\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    207\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    208\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:398\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 398\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_java(dataset)\n\u001B[1;32m    399\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py:395\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 395\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj\u001B[38;5;241m.\u001B[39mfit(dataset\u001B[38;5;241m.\u001B[39m_jdf)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1356\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1357\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1358\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1359\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1361\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1362\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1363\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1366\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:275\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    271\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    274\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 275\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    277\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mIllegalArgumentException\u001B[0m: [FIELD_NOT_FOUND] No such struct field `weight` in `full_unscaled_features`, `ARR_DEL15`. SQLSTATE: 42704"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_parameters = rf_results_full_features['best_parameters']\n",
    "\n",
    "print(f\"Best Parameters: {best_parameters}\")\n",
    "print(f\"Feature column: full_unscaled_features\")\n",
    "\n",
    "pipeline_full_unscaled_features = get_rf_model_with_prebuilt_features(\n",
    "    best_parameters, \n",
    "    feature_col='full_unscaled_features', \n",
    "    use_weights=True\n",
    ")\n",
    "\n",
    "columns_to_keep_full_unscaled_only = ['full_unscaled_features', 'ARR_DEL15']\n",
    "\n",
    "# =======================\n",
    "# USE DOWNSAMPLED TRAINING DATA\n",
    "# =======================\n",
    "train_full_df = df_train_master.select(columns_to_keep_full_unscaled_only)\n",
    "print(f\"\\nTotal training rows: {train_full_df.count():,}\")\n",
    "\n",
    "print(f\"Class distribution in training data:\")\n",
    "train_full_df.groupBy('ARR_DEL15').count().show()\n",
    "\n",
    "# =======================\n",
    "# TRAIN MODEL\n",
    "# =======================\n",
    "print(f\"Training model on downsampled data...\")\n",
    "model_full_unscaled_features = pipeline_full_unscaled_features.fit(train_full_df)\n",
    "\n",
    "# =======================\n",
    "# TEST ON BLIND TEST SET\n",
    "# =======================\n",
    "test_df = spark.read.parquet(blind_test_set_path).select(columns_to_keep_full_unscaled_only)\n",
    "print(f\"Total test samples: {test_df.count():,}\")\n",
    "print(f\"Running predictions...\")\n",
    "\n",
    "test_pred_full_unscaled_features = model_full_unscaled_features.transform(test_df)\n",
    "\n",
    "f2, pr_auc, precision, recall, tp, fp, fn, tn = cv_eval(test_pred_full_unscaled_features)\n",
    "\n",
    "print(\"\\nBLIND TEST SET RESULTS (full_unscaled_features):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"F2 Score:  {f2:.4f}\")\n",
    "print(f\"PR-AUC:    {pr_auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TP={tp:,}, FP={fp:,}\")\n",
    "print(f\"  FN={fn:,}, TN={tn:,}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8dbedc-d344-4616-94bf-b31e118761f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Blind Test on Log_Unscaled_Features only"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:913)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:939)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:938)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:993)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:778)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:913)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:939)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:938)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:993)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:778)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_parameters = rf_results_log_features['best_parameters']\n",
    "\n",
    "print(f\"Best Parameters: {best_parameters}\")\n",
    "print(f\"Feature column: log_unscaled_features\")\n",
    "\n",
    "pipeline_log_unscaled_features = get_rf_model_with_prebuilt_features(\n",
    "    best_parameters, \n",
    "    feature_col='log_unscaled_features', \n",
    "    use_weights=False\n",
    ")\n",
    "\n",
    "columns_to_keep_log_unscaled_only = ['log_unscaled_features', 'ARR_DEL15']\n",
    "\n",
    "print(f\"\\nUsing downsampled training data\")\n",
    "train_full_df_log_unscaled_only = df_train_master.select(columns_to_keep_log_unscaled_only)\n",
    "\n",
    "print(f\"Total training samples: {train_full_df_log_unscaled_only.count():,}\")\n",
    "print(f\"Training final model...\")\n",
    "\n",
    "model_log_unscaled_features = pipeline_log_unscaled_features.fit(train_full_df_log_unscaled_only)\n",
    "\n",
    "test_df = spark.read.parquet(blind_test_set_path)\n",
    "test_df_log_unscaled_only = test_df.select(columns_to_keep_log_unscaled_only)\n",
    "\n",
    "print(f\"Total test samples: {test_df_log_unscaled_only.count():,}\")\n",
    "print(f\"Running predictions...\")\n",
    "\n",
    "test_pred_log_unscaled_features = model_log_unscaled_features.transform(test_df_log_unscaled_only)\n",
    "\n",
    "f2, pr_auc, precision, recall, tp, fp, fn, tn = cv_eval(test_pred_log_unscaled_features)\n",
    "\n",
    "print(\"\\nBLIND TEST SET RESULTS (log_unscaled_features):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"F2 Score:  {f2:.4f}\")\n",
    "print(f\"PR-AUC:    {pr_auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TP={tp:,}, FP={fp:,}\")\n",
    "print(f\"  FN={fn:,}, TN={tn:,}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b93f284e-7a46-4b31-b900-ae2ddcd16bf2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Full_Unscaled vs Log_Unscaled Comparison"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: full_unscaled_features vs log_unscaled_features\")\n",
    "print(\"=\"*80)\n",
    "print(f\"full_unscaled_features - F2: {rf_results_full_features['best_f2_mean']:.4f}, Recall: {rf_results_full_features['best_recall_mean']:.4f}\")\n",
    "print(f\"log_unscaled_features  - F2: {rf_results_log_features['best_f2_mean']:.4f}, Recall: {rf_results_log_features['best_recall_mean']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23d17fe-1b58-405b-98f1-91b8265ec4b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Archived Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f19d45-eef2-4cc8-a087-7d7e217c6815",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train on Log_Unscaled_Features"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:913)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:939)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:938)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:993)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:778)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:434)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:473)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:750)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:84)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:84)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:728)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:913)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$4(Chauffeur.scala:939)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:938)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:993)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:778)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1033)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:953)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:548)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:513)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$13(ActivityContextFactory.scala:831)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$3(ActivityContextFactory.scala:831)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:794)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:776)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$15(ActivityContextFactory.scala:285)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:53)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:285)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:513)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:408)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%skip\n",
    "rf_results_log_features = model_eval_on_folds_prebuilt(\n",
    "    folds_input_path=folds_input_path,\n",
    "    param_grid=param_grid,\n",
    "    feature_col='log_unscaled_features',\n",
    "    N_SPLITS=N_SPLITS,\n",
    "    use_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f32f45a-e1bb-4b2f-bcd2-8ffddb239467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Function to summarize null/NaN values for all columns\n",
    "def summarize_nulls_all(df):\n",
    "    \"\"\"\n",
    "    Summarize null/NaN values for all columns\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with null value statistics\n",
    "    \"\"\"\n",
    "    null_stats = []\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        null_percent = (null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "        \n",
    "        null_stats.append({\n",
    "            'Column': column,\n",
    "            'Null_Count': null_count,\n",
    "            'Null_Percent': round(null_percent, 2),\n",
    "            'Non_Null_Count': total_rows - null_count\n",
    "        })\n",
    "    \n",
    "    return spark.createDataFrame(null_stats)\n",
    "\n",
    "# Get null summary for all columns\n",
    "null_summary = summarize_nulls_all(sample_fold)\n",
    "\n",
    "# Sort by null percentage (descending) to see which columns have the most missing data\n",
    "null_summary.orderBy(col('Null_Percent').desc()).show(truncate=False, n=len(sample_fold.columns))\n",
    "\n",
    "# Optional: Filter columns with 0% nulls (completely non-null)\n",
    "non_null_cols = null_summary.filter(col('Null_Percent') == 0).select('Column').rdd.flatMap(lambda x: x).collect()\n",
    "print(f\"\\nColumns with NO null values ({len(non_null_cols)} columns):\")\n",
    "for col_name in sorted(non_null_cols):\n",
    "    print(f\"  - {col_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79ccd77b-5473-4490-92ed-29b77dccb704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "cols_to_int_clean = [c for c in cols_to_int if c not in leakage_cols and c in cols_no_nulls]\n",
    "cols_to_double_clean = [c for c in cols_to_double if c not in leakage_cols and c in cols_no_nulls]\n",
    "binary_cols_clean = [c for c in binary_cols if c not in leakage_cols and c in cols_no_nulls]\n",
    "\n",
    "numeric_cols_clean = cols_to_int_clean + cols_to_double_clean + binary_cols_clean\n",
    "\n",
    "feature_eng_numeric = [\n",
    "    \"TAXI_OUT_log_imp\", \"TAXI_IN_log_imp\", \"CRS_ELAPSED_TIME_log_imp\", \n",
    "    \"AIR_TIME_log_imp\", \"DISTANCE_log_imp\", \"ORIGIN_ELEVATION_FT_log_imp\", \n",
    "    \"DEST_ELEVATION_FT_log_imp\", \"LowestBase_ft_agl_log_imp\", \"BaseRange_ft_agl_log_imp\", \n",
    "    \"Ceiling_ft_agl_log_imp\", \"HourlyStationPressure_log_imp\", \"HourlyVisibility_log_imp\", \n",
    "    \"HourlyPrecipitation_log_imp\",\"is_us_holiday\"\n",
    "]\n",
    "numeric_cols_clean.extend([c for c in feature_eng_numeric if c in cols_no_nulls and c not in numeric_cols_clean])\n",
    "\n",
    "from pyspark.sql.functions import stddev\n",
    "\n",
    "print(f\"NUMERIC COLUMN VARIANCE ANALYSIS\")\n",
    "print(f\"-\" * 67)\n",
    "print(f\"Total numeric columns to assess: {len(numeric_cols_clean)}\\n\")\n",
    "\n",
    "numeric_std = df_sample.select([stddev(c).alias(c) for c in numeric_cols_clean]).collect()[0].asDict()\n",
    "\n",
    "variance_results = [(col, std_val) for col, std_val in numeric_std.items()]\n",
    "variance_results.sort(key=lambda x: x[1] if x[1] is not None else 0)\n",
    "\n",
    "print(f\"{'Column':<50} {'Std Dev':>15}\")\n",
    "print(\"-\" * 67)\n",
    "for col, std_val in variance_results:\n",
    "    if std_val is not None:\n",
    "        print(f\"{col:<50} {std_val:>15.6f}\")\n",
    "    else:\n",
    "        print(f\"{col:<50} {'None':>15}\")\n",
    "\n",
    "variance_threshold = 0.01\n",
    "low_variance = [(col, std) for col, std in variance_results if std is not None and std <= variance_threshold]\n",
    "high_variance = [(col, std) for col, std in variance_results if std is not None and std > variance_threshold]\n",
    "\n",
    "print(f\"\\n\" + \"-\" * 67)\n",
    "print(f\"RECOMMENDATIONS (variance threshold: {variance_threshold})\")\n",
    "print(f\"-\" * 67)\n",
    "\n",
    "if low_variance:\n",
    "    print(f\"\\nLOW VARIANCE (<={variance_threshold}) - Consider excluding:\")\n",
    "    for col, std in low_variance:\n",
    "        print(f\"  {col:<50} {std:>15.6f}\")\n",
    "\n",
    "print(f\"\\nHIGH VARIANCE (>{variance_threshold}) - Safe to use:\")\n",
    "print(f\"  {len(high_variance)} columns\")\n",
    "\n",
    "print(f\"\\nTotal columns to exclude: {len(low_variance)}\")\n",
    "print(f\"Total columns to keep: {len(high_variance)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61d77333-6dbb-48ac-b882-5e1515eeca0d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Train on Full_Unscaled_Features"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "rf_results_full_features = model_eval_on_folds_prebuilt(\n",
    "    folds_input_path=folds_input_path,\n",
    "    param_grid=param_grid,\n",
    "    feature_col='full_unscaled_features',\n",
    "    N_SPLITS=N_SPLITS,\n",
    "    use_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6781b331-b815-4a44-820c-05f8eab5f8a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "print(\"=\" * 80)\n",
    "print(\"TRIPLE CHECK: FEATURE EXISTENCE ACROSS ALL WINDOWS + TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# All features to check\n",
    "all_features_to_check = numeric_cols_to_use + cat_cols_to_use\n",
    "\n",
    "print(f\"\\nChecking {len(all_features_to_check)} features:\")\n",
    "print(f\"  Numeric: {len(numeric_cols_to_use)}\")\n",
    "print(f\"  Categorical: {len(cat_cols_to_use)}\")\n",
    "\n",
    "# Check across all training windows\n",
    "print(f\"\\nChecking across {N_SPLITS} training windows...\")\n",
    "all_windows_have_features = True\n",
    "\n",
    "for i in range(1, N_SPLITS + 1):\n",
    "    window_df = spark.read.parquet(f\"{folds_input_path}/window_{i}_train\")\n",
    "    window_cols = set(window_df.columns)\n",
    "    \n",
    "    missing_in_window = [col for col in all_features_to_check if col not in window_cols]\n",
    "    \n",
    "    if missing_in_window:\n",
    "        all_windows_have_features = False\n",
    "        print(f\"  ✗ Window {i}: MISSING {len(missing_in_window)} features: {missing_in_window}\")\n",
    "    else:\n",
    "        print(f\"  ✓ Window {i}: All features present\")\n",
    "\n",
    "# Check test set\n",
    "print(f\"\\nChecking test set...\")\n",
    "df_test = spark.read.parquet(test_set_path)\n",
    "test_cols = set(df_test.columns)\n",
    "\n",
    "missing_in_test = [col for col in all_features_to_check if col not in test_cols]\n",
    "\n",
    "if missing_in_test:\n",
    "    print(f\"  ✗ Test set: MISSING {len(missing_in_test)} features:\")\n",
    "    for col in missing_in_test:\n",
    "        col_type = \"numeric\" if col in numeric_cols_to_use else \"categorical\"\n",
    "        print(f\"    - {col} ({col_type})\")\n",
    "else:\n",
    "    print(f\"  ✓ Test set: All features present\")\n",
    "\n",
    "# Final verdict\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if all_windows_have_features and not missing_in_test:\n",
    "    print(\"✅ SUCCESS: ALL FEATURES EXIST IN ALL WINDOWS AND TEST SET\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Ready to train with {len(all_features_to_check)} features:\")\n",
    "    print(f\"  - {len(numeric_cols_to_use)} numeric features\")\n",
    "    print(f\"  - {len(cat_cols_to_use)} categorical features\")\n",
    "    print(\"\\n\uD83D\uDE80 You can now proceed with model training!\")\n",
    "else:\n",
    "    print(\"❌ FAILURE: SOME FEATURES ARE MISSING\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"⚠️  You MUST fix the feature list before training!\")\n",
    "    \n",
    "    if not all_windows_have_features:\n",
    "        print(\"\\n  Problem: Some features missing from training windows\")\n",
    "    if missing_in_test:\n",
    "        print(\"\\n  Problem: Some features missing from test set\")\n",
    "        print(f\"\\n  Missing from test: {missing_in_test}\")\n",
    "        print(\"\\n  Solution: Remove these features from your feature lists:\")\n",
    "        print(f\"    numeric_cols_to_use = [c for c in numeric_cols_to_use if c not in {missing_in_test}]\")\n",
    "        print(f\"    cat_cols_to_use = [c for c in cat_cols_to_use if c not in {missing_in_test}]\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1530ecb-e8c6-4656-9ae1-c2d70e0325cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": "Final Alignment Check"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "test_final_cols = set(df_test_final_clean.columns)\n",
    "\n",
    "# Collect columns from ALL training windows (clean, no artifacts)\n",
    "window_clean_cols = {}\n",
    "for w in train_windows:\n",
    "    path = f\"{folds_input_path}/{w}\"\n",
    "    df_window = spark.read.parquet(path)\n",
    "    \n",
    "    # Remove artifact columns\n",
    "    artifact_cols = [c for c in df_window.columns if \n",
    "                     c.endswith('_ohe') or \n",
    "                     c.endswith('_idx') or \n",
    "                     c.endswith('_log') or \n",
    "                     c in ['features_num', 'features_num_scaled']]\n",
    "    \n",
    "    clean_cols = set([c for c in df_window.columns if c not in artifact_cols])\n",
    "    window_clean_cols[w] = clean_cols\n",
    "    print(f\"{w}: {len(clean_cols)} clean columns\")\n",
    "\n",
    "# Find columns common to ALL windows\n",
    "common_across_all_windows = set.intersection(*window_clean_cols.values())\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Columns common to ALL {len(train_windows)} training windows: {len(common_across_all_windows)}\")\n",
    "\n",
    "# Compare test vs common training columns\n",
    "common_test_train = test_final_cols & common_across_all_windows\n",
    "only_in_test = test_final_cols - common_across_all_windows\n",
    "only_in_all_windows = common_across_all_windows - test_final_cols\n",
    "\n",
    "print(f\"\\nAlignment:\")\n",
    "print(f\"  Columns in BOTH test and ALL windows: {len(common_test_train)}\")\n",
    "print(f\"  Columns ONLY in test: {len(only_in_test)}\")\n",
    "print(f\"  Columns ONLY in ALL windows: {len(only_in_all_windows)}\")\n",
    "\n",
    "if only_in_test:\n",
    "    print(f\"\\n⚠️ Columns ONLY in test:\")\n",
    "    for col in sorted(only_in_test):\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "if only_in_all_windows:\n",
    "    print(f\"\\n⚠️ Columns ONLY in ALL training windows:\")\n",
    "    for col in sorted(only_in_all_windows):\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# Check model features specifically\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"MODEL FEATURES ACROSS ALL WINDOWS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "model_features_in_test = [c for c in all_model_features if c in test_final_cols]\n",
    "model_features_in_all_windows = [c for c in all_model_features if c in common_across_all_windows]\n",
    "model_features_common = [c for c in all_model_features if c in common_test_train]\n",
    "\n",
    "print(f\"Total model features required: {len(all_model_features)}\")\n",
    "print(f\"  Present in test: {len(model_features_in_test)}\")\n",
    "print(f\"  Present in ALL windows: {len(model_features_in_all_windows)}\")\n",
    "print(f\"  Present in BOTH test AND all windows: {len(model_features_common)}\")\n",
    "\n",
    "missing_from_test = [c for c in all_model_features if c not in test_final_cols]\n",
    "missing_from_windows = [c for c in all_model_features if c not in common_across_all_windows]\n",
    "\n",
    "if missing_from_test:\n",
    "    print(f\"\\n⚠️ Model features missing from test:\")\n",
    "    for col in missing_from_test:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "if missing_from_windows:\n",
    "    print(f\"\\n⚠️ Model features missing from some training windows:\")\n",
    "    for col in missing_from_windows:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# Final verdict\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"ULTIMATE FINAL VERDICT\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "all_checks_passed = (\n",
    "    len(only_in_test) == 0 and \n",
    "    len(only_in_all_windows) == 0 and \n",
    "    len(missing_from_test) == 0 and \n",
    "    len(missing_from_windows) == 0 and\n",
    "    len(model_features_common) == len(all_model_features)\n",
    ")\n",
    "\n",
    "if all_checks_passed:\n",
    "    print(f\"✅✅✅ PERFECT ALIGNMENT!\")\n",
    "    print(f\"\\n\uD83D\uDCCA df_test_final_clean specifications:\")\n",
    "    print(f\"   - Rows: {df_test_final_clean.count():,}\")\n",
    "    print(f\"   - Columns: {len(df_test_final_clean.columns)}\")\n",
    "    print(f\"   - Model features: {len(model_features_common)}\")\n",
    "    print(f\"   - Nulls: 0\")\n",
    "    print(f\"   - Aligned with ALL {len(train_windows)} training windows: ✅\")\n",
    "    print(f\"\\n\uD83C\uDFAF\uD83C\uDFAF\uD83C\uDFAF READY FOR RANDOM FOREST PREDICTIONS!\")\n",
    "else:\n",
    "    print(f\"⚠️ Issues detected - review details above\")\n",
    "    print(f\"\\nSummary of issues:\")\n",
    "    if only_in_test:\n",
    "        print(f\"  - {len(only_in_test)} columns only in test\")\n",
    "    if only_in_all_windows:\n",
    "        print(f\"  - {len(only_in_all_windows)} columns only in training\")\n",
    "    if missing_from_test:\n",
    "        print(f\"  - {len(missing_from_test)} model features missing from test\")\n",
    "    if missing_from_windows:\n",
    "        print(f\"  - {len(missing_from_windows)} model features missing from windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d54995-94e3-43ff-b8a0-26737b7a32e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Function to summarize null/NaN values\n",
    "def summarize_nulls(df, cols):\n",
    "    null_stats = []\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    for column in cols:\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        null_percent = (null_count / total_rows) * 100 if total_rows > 0 else 0\n",
    "        \n",
    "        null_stats.append({\n",
    "            'Column': column,\n",
    "            'Null_Count': null_count,\n",
    "            'Null_Percent': round(null_percent, 2),\n",
    "            'Non_Null_Count': total_rows - null_count\n",
    "        })\n",
    "    \n",
    "    return spark.createDataFrame(null_stats)\n",
    "\n",
    "# Get null summary\n",
    "all_selected_cols = numeric_cols_to_use + cat_cols_ohe\n",
    "null_summary = summarize_nulls(sample_fold, all_selected_cols)\n",
    "\n",
    "# Sort by null percentage (descending)\n",
    "print(\"=\" * 80)\n",
    "print(\"NULL VALUE SUMMARY FOR SELECTED FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "null_summary.orderBy(col('Null_Percent').desc()).show(truncate=False, n=len(all_selected_cols))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "cols_with_nulls = null_summary.filter(col('Null_Percent') > 0).count()\n",
    "cols_no_nulls = null_summary.filter(col('Null_Percent') == 0).count()\n",
    "\n",
    "print(f\"Total columns selected: {len(all_selected_cols)}\")\n",
    "print(f\"Columns with NO nulls: {cols_no_nulls}\")\n",
    "print(f\"Columns WITH nulls: {cols_with_nulls}\")\n",
    "\n",
    "if cols_with_nulls > 0:\n",
    "    print(\"\\nColumns with nulls:\")\n",
    "    null_summary.filter(col('Null_Percent') > 0).orderBy(col('Null_Percent').desc()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166a0387-f07d-4256-bdee-277e131de8e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# Filter both numeric and categorical columns to what's actually available\n",
    "sample_fold = spark.read.parquet(f\"{folds_input_path}/fold_1_train\")\n",
    "available_cols = set(sample_fold.columns)\n",
    "\n",
    "# Filter numeric columns (exclude CANCELLED, DIVERTED)\n",
    "numeric_cols_to_use = [col for col in numeric_cols_clean if col in available_cols]\n",
    "\n",
    "# Filter categorical columns (only keep those with _ohe versions)\n",
    "cat_cols_to_use = [col for col in cat_cols if f\"{col}_ohe\" in available_cols]\n",
    "\n",
    "print(f\"Using {len(numeric_cols_to_use)} numeric columns\")\n",
    "print(f\"Using {len(cat_cols_to_use)} categorical columns\")\n",
    "print(f\"Excluded categorical: DEP_TIME_BLK, ARR_TIME_BLK\")\n",
    "\n",
    "# Then in your grid search loop:\n",
    "pipeline = get_rf_model(param_print, cat_cols_to_use, numeric_cols_to_use, use_weights=use_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed334a71-e7ba-4351-87d6-95861f05a1b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "def get_rf_model(params, cat_cols, numeric_cols, use_weights=False):\n",
    "    \"\"\"\n",
    "    Create Random Forest pipeline for ALREADY PREPROCESSED data\n",
    "    Assumes: StringIndexer and OneHotEncoder already applied in scaled_smote_folds\n",
    "    \n",
    "    Args:\n",
    "        params: dict with 'maxDepth', 'numTrees', 'maxBins'\n",
    "        cat_cols: list of categorical column names (base names, _ohe will be added)\n",
    "        numeric_cols: list of numeric column names (already scaled/cleaned)\n",
    "        use_weights: bool, whether to use class weights (for SAMPLING_METHOD='weights')\n",
    "    \n",
    "    Returns:\n",
    "        Pipeline with VectorAssembler + RandomForest only\n",
    "    \"\"\"\n",
    "    from pyspark.ml.classification import RandomForestClassifier\n",
    "    \n",
    "    # Stage 1: VectorAssembler - combine numeric + categorical features\n",
    "    # Assumes _ohe columns already exist from preprocessing\n",
    "    final_assembler = VectorAssembler(\n",
    "        inputCols=numeric_cols + [f\"{col}_ohe\" for col in cat_cols], \n",
    "        outputCol=\"features\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    # Stage 2: Random Forest classifier\n",
    "    rf_params = {\n",
    "        'labelCol': \"ARR_DEL15\",\n",
    "        'featuresCol': \"features\",\n",
    "        'maxDepth': params['maxDepth'],\n",
    "        'numTrees': params['numTrees'],\n",
    "        'maxBins': params.get('maxBins', 32),\n",
    "        'seed': 42\n",
    "    }\n",
    "    \n",
    "    # Add weight column if using class weights\n",
    "    if use_weights:\n",
    "        rf_params['weightCol'] = 'weight'\n",
    "    \n",
    "    rf = RandomForestClassifier(**rf_params)\n",
    "    \n",
    "    # Return pipeline with only 2 stages\n",
    "    pipeline = Pipeline(stages=[final_assembler, rf])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ed8af6-5951-443e-8db3-09a5a92c31f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "null_check = df_test_final.filter(\n",
    "    F.col(\"DEST_LAT\").isNull() | \n",
    "    F.col(\"DEST_LON\").isNull() | \n",
    "    F.col(\"DEST_ELEVATION_FT\").isNull() |\n",
    "    F.col(\"DEST_SIZE\").isNull()\n",
    ")\n",
    "\n",
    "print(f\"Rows with destination nulls: {null_check.count()}\")\n",
    "print(f\"Checking if all 4 columns are null together...\")\n",
    "\n",
    "# Check if they're all null together\n",
    "all_dest_null = null_check.filter(\n",
    "    F.col(\"DEST_LAT\").isNull() & \n",
    "    F.col(\"DEST_LON\").isNull() & \n",
    "    F.col(\"DEST_ELEVATION_FT\").isNull() &\n",
    "    F.col(\"DEST_SIZE\").isNull()\n",
    ").count()\n",
    "\n",
    "print(f\"Rows where all 4 DEST columns are null: {all_dest_null}\")\n",
    "\n",
    "# Option 1: Drop these rows (recommended if only 334 out of 1.5M)\n",
    "print(f\"\\nOption 1: Drop rows with null DEST columns\")\n",
    "df_test_final_clean = df_test_final.filter(\n",
    "    F.col(\"DEST_LAT\").isNotNull() & \n",
    "    F.col(\"DEST_LON\").isNotNull() & \n",
    "    F.col(\"DEST_ELEVATION_FT\").isNotNull() &\n",
    "    F.col(\"DEST_SIZE\").isNotNull()\n",
    ")\n",
    "\n",
    "print(f\"  Before: {df_test_final.count():,} rows\")\n",
    "print(f\"  After: {df_test_final_clean.count():,} rows\")\n",
    "print(f\"  Dropped: {df_test_final.count() - df_test_final_clean.count()} rows ({((df_test_final.count() - df_test_final_clean.count()) / df_test_final.count() * 100):.3f}%)\")\n",
    "\n",
    "final_null_check = (\n",
    "    df_test_final_clean.select([\n",
    "        F.sum(F.col(c).isNull().cast(\"int\")).alias(c)\n",
    "        for c in present_in_test\n",
    "    ])\n",
    "    .collect()[0]\n",
    "    .asDict()\n",
    ")\n",
    "\n",
    "remaining_nulls = {c: n for c, n in final_null_check.items() if n > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6bfb60e-313a-4547-8b48-22bf0aa2e977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "def parameter_sets(param_grid):\n",
    "    \"\"\"Generate all combinations of parameters from param_grid\"\"\"\n",
    "    from itertools import product\n",
    "    parameter_names = list(param_grid.keys())\n",
    "    parameter_values = list(param_grid.values())\n",
    "    parameters = list(product(*parameter_values))\n",
    "    return parameter_names, parameters\n",
    "  \n",
    "param_grid = {\n",
    "    'maxDepth': [5, 10],\n",
    "    'numTrees': [50],\n",
    "    'maxBins': [32]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e74979-09d5-4613-8822-96ab29b582ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "#Leakage column\n",
    "leakage_cols = ['ARR_DELAY','DEP_DELAY','CARRIER_DELAY','WEATHER_DELAY', 'NAS_DELAY','SECURITY_DELAY','LATE_AIRCRAFT_DELAY','DEP_DEL15','ACTUAL_ELAPSED_TIME','ARR_TIME','AIR_TIME','TAXI_IN','TAXI_OUT', 'WHEELS_OFF','WHEELS_ON']\n",
    "cols_to_int_clean = [c for c in cols_to_int if c not in leakage_cols]\n",
    "cols_to_double_clean = [c for c in cols_to_double if c not in leakage_cols]\n",
    "binary_cols_clean = [c for c in binary_cols if c not in leakage_cols and c != 'ARR_DEL15']\n",
    "\n",
    "# Combine clean numeric columns\n",
    "numeric_cols_clean = cols_to_int_clean + cols_to_double_clean + binary_cols_clean\n",
    "\n",
    "print(f\"Original numeric columns: {len(cols_to_int + cols_to_double + binary_cols)}\")\n",
    "print(f\"Clean numeric columns (after removing leakage): {len(numeric_cols_clean)}\")\n",
    "print(f\"Removed columns: {leakage_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35dccdb-9969-4a41-9f71-53df8d2395b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "SAMPLING_METHOD = 'weights'\n",
    "N_SPLITS = 4\n",
    "metric = \"f2\"\n",
    "best_score = 0\n",
    "best_param_vals = None\n",
    "best_parameters = None\n",
    "best_model = None\n",
    "best_predictions = None\n",
    "parameter_names, parameters = parameter_sets(param_grid)\n",
    "\n",
    "print(f'Number of folds: {N_SPLITS}')\n",
    "print(f'Total parameter combinations to test: {len(parameters)}')\n",
    "print(\"*\"*10)\n",
    "\n",
    "if len(parameters) == 1:\n",
    "    print('Only 1 parameter')\n",
    "\n",
    "# Loop through all parameter combinations\n",
    "for p in parameters:\n",
    "    param_print = {x[0]:x[1] for x in zip(parameter_names, p)}\n",
    "    print(f\"Parameters: {param_print}\")  \n",
    "    scores = []\n",
    "  \n",
    "    for i in range(N_SPLITS):\n",
    "        # Load Folds (from scaled_smote_folds - already preprocessed)\n",
    "        train_path = f\"{folds_input_path}/fold_{i+1}_train\"\n",
    "        val_path = f\"{folds_input_path}/fold_{i+1}_val\"\n",
    "        train_df = spark.read.parquet(train_path)\n",
    "        dev_df = spark.read.parquet(val_path)\n",
    "        \n",
    "\n",
    "        # Determine if we're using weights\n",
    "        use_weights = (SAMPLING_METHOD == 'weights')\n",
    "        \n",
    "        # Apply sampling (if needed - may already be in scaled_smote_folds)\n",
    "        if SAMPLING_METHOD == 'down':\n",
    "            train_df = downsample(train_df)\n",
    "        elif SAMPLING_METHOD == 'up':\n",
    "            train_df = upsample(train_df)\n",
    "        elif SAMPLING_METHOD == 'weights':\n",
    "            train_df = add_class_weights(train_df)\n",
    "        \n",
    "        train_df = train_df.cache()\n",
    "        \n",
    "        print(f'    TRAIN set for fold {i+1} count is {train_df.count():,} flights ({SAMPLING_METHOD})' if SAMPLING_METHOD else f'    TRAIN set for fold {i+1} count is {train_df.count():,} flights (no sampling)')\n",
    "        print(f'    DEV set for fold {i+1} count is {dev_df.count():,} flights')\n",
    "        \n",
    "        # Get pipeline with weights support\n",
    "        pipeline = get_rf_model(param_print, cat_cols_to_use, numeric_cols_to_use, use_weights=use_weights)\n",
    "        \n",
    "        # Fit and transform\n",
    "        model = pipeline.fit(train_df)\n",
    "        dev_pred = model.transform(dev_df)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if metric == 'f2':\n",
    "            score = cv_eval(dev_pred)[0]\n",
    "        elif metric == 'pr':\n",
    "            score = cv_eval(dev_pred)[1]\n",
    "        \n",
    "        scores.append(score)\n",
    "        print(f'    Fold {i+1} {metric} score: {score:.2f}')\n",
    "        print('------------------------------------------------------------')\n",
    "        \n",
    "        if best_param_vals == None:\n",
    "            best_param_vals = p\n",
    "        \n",
    "        # Store best predictions for PR curve\n",
    "        if score > best_score:\n",
    "            best_predictions = dev_pred\n",
    "    \n",
    "    # Take average of all scores\n",
    "    avg_score = np.average(scores)\n",
    "    \n",
    "    # Update best score\n",
    "    if avg_score > best_score:\n",
    "        previous_best = best_score\n",
    "        best_score = avg_score\n",
    "        best_parameters = param_print\n",
    "        best_param_vals = p\n",
    "        print(f'new best score of {best_score:.2f}')\n",
    "    else:\n",
    "        print(f'Result was not better, score was {avg_score:.2f} with best {metric} score {best_score:.2f}')\n",
    "    print(\"************************************************************\")\n",
    "\n",
    "print('')\n",
    "print('='*20)\n",
    "print('Grid Search Done')\n",
    "print('='*20)\n",
    "print('Best parameter:')\n",
    "print(best_parameters)\n",
    "print(f'Best average {metric} score: {best_score:.2f}')\n",
    "print(f'Sampling method: {SAMPLING_METHOD}')\n",
    "print('='*80)\n",
    "\n",
    "# Save results\n",
    "dbutils.fs.mkdirs(chads_test_output)\n",
    "results_df = spark.createDataFrame(\n",
    "    [(str(best_parameters), float(best_score), SAMPLING_METHOD)], \n",
    "    [\"best_params\", \"best_score\", \"sampling_method\"]\n",
    ")\n",
    "results_df.write.mode(\"overwrite\").parquet(f\"{chads_test_output}/grid_search_results\")\n",
    "print(f\"Results saved to: {chads_test_output}/grid_search_results\")\n",
    "\n",
    "# Plot PR curve for best model\n",
    "if best_predictions is not None:\n",
    "    print(\"\\nPlotting PR curve for best model...\")\n",
    "    plot_pr_curve(best_predictions, title=f\"PR Curve - Best Model (F2={best_score:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62c4ae30-77fa-4ad4-9ad2-9175a3378036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "def evaluate_baseline_on_folds(folds_input_path, N_SPLITS=4, plot_pr=plot_pr):\n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    scores_f2 = []\n",
    "    scores_pr = []\n",
    "\n",
    "    for i in range(N_SPLITS):\n",
    "        print(f\"\\nFold {i+1}\")\n",
    "\n",
    "        # Load data\n",
    "        train_df = spark.read.parquet(f\"{folds_input_path}/fold_{i+1}_train\")\n",
    "        val_df = spark.read.parquet(f\"{folds_input_path}/fold_{i+1}_val\")\n",
    "\n",
    "        # Majority class baseline\n",
    "        majority_class = (\n",
    "            train_df.groupBy(\"ARR_DEL15\")\n",
    "            .count()\n",
    "            .orderBy(F.desc(\"count\"))\n",
    "            .first()[0]\n",
    "        )\n",
    "\n",
    "        preds_baseline = (\n",
    "            val_df\n",
    "            .withColumn(\"prediction\", F.lit(float(majority_class)))\n",
    "            .withColumn(\"probability\", prob_udf(F.lit(majority_class)))\n",
    "        )\n",
    "\n",
    "        # Compute metrics\n",
    "        f2, pr_auc = cv_eval(preds_baseline)\n",
    "        scores_f2.append(f2)\n",
    "        scores_pr.append(pr_auc)\n",
    "\n",
    "        print(f\"Fold {i+1} — F2: {f2:.4f}, PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"f2_scores\": scores_f2,\n",
    "        \"pr_auc_scores\": scores_pr,\n",
    "        \"f2_mean\": float(np.mean(scores_f2)),\n",
    "        \"f2_std\": float(np.std(scores_f2)),\n",
    "        \"pr_auc_mean\": float(np.mean(scores_pr)),\n",
    "        \"pr_auc_std\": float(np.std(scores_pr)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e9c8ced-a1ec-4dcb-a307-966e344e73ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "#Checkpoint location\n",
    "dbfs_path = \"dbfs:/student-groups/Group_02_01\"\n",
    "\n",
    "#RUN FOR 3 MONTH FOLDS\n",
    "time = 3 #test on 3 months\n",
    "\n",
    "# Define your existing paths\n",
    "time_length = data_set(time)\n",
    "splits_path = f\"{dbfs_path}/splits{time_length}\"\n",
    "folds_input_path = f\"{splits_path}/scaled_smote_folds\" #use scaled_smote_folds\n",
    "\n",
    "#run evaluation\n",
    "baseline_results_3m = evaluate_baseline_on_folds(\n",
    "    folds_input_path=folds_input_path,\n",
    "    N_SPLITS=4,\n",
    "    plot_pr=False   #  <- don't plot PR-AUC curves (save time)\n",
    ")\n",
    "\n",
    "baseline_results_3m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6afaf3b8-2aa5-4a10-aaac-d19e4f69fde0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "#RUN FOR 12 MONTH \n",
    "time = 12 #1 yr\n",
    "\n",
    "# Define your existing paths\n",
    "time_length = data_set(time)\n",
    "splits_path = f\"{dbfs_path}/splits{time_length}\"\n",
    "folds_input_path = f\"{splits_path}/scaled_smote_folds\" #use scaled_smote_folds\n",
    "\n",
    "#run evaluation\n",
    "baseline_results_1yr = evaluate_baseline_on_folds(\n",
    "    folds_input_path=folds_input_path,\n",
    "    N_SPLITS=4,\n",
    "    plot_pr=True   #  <- enable PR-AUC curves\n",
    ")\n",
    "\n",
    "baseline_results_1yr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "756f9571-58f4-437b-8a6e-5d1b1d7425b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "#Checkpoint location\n",
    "dbfs_path = \"dbfs:/student-groups/Group_02_01\"\n",
    "\n",
    "#RUN FOR 1 year\n",
    "time = 12 #1 year\n",
    "\n",
    "time_length = data_set(time)\n",
    "splits_path = f\"{dbfs_path}/splits{time_length}\"\n",
    "\n",
    "# Define your existing paths\n",
    "folds_input_path = f\"{splits_path}/scaled_smote_folds\" #use scaled_smote_folds\n",
    "N_SPLITS = 4  # number of folds\n",
    "plot_pr = True #plot PR-AUC curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "732e102a-149e-40bc-8c24-3bb9fb28ab57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "scores_f2 = []\n",
    "scores_pr = []\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "    print(f\"\\nFold {i+1}\")\n",
    "    \n",
    "    train_path = f\"{folds_input_path}/fold_{i+1}_train\"\n",
    "    val_path = f\"{folds_input_path}/fold_{i+1}_val\"\n",
    "    \n",
    "    train_df = spark.read.parquet(train_path)\n",
    "    val_df = spark.read.parquet(val_path)\n",
    "    \n",
    "    # Majority class baseline\n",
    "    majority_class = (\n",
    "        train_df.groupBy(\"ARR_DEL15\")\n",
    "        .count()\n",
    "        .orderBy(F.desc(\"count\"))\n",
    "        .first()[0]\n",
    "    )\n",
    "    \n",
    "    print(f\"Majority class (Fold {i+1}): {majority_class}\")\n",
    "\n",
    "    preds_baseline = (\n",
    "        val_df\n",
    "        .withColumn(\"prediction\", F.lit(float(majority_class)))\n",
    "        .withColumn(\"probability\", prob_udf(F.lit(majority_class)))\n",
    "    )\n",
    "\n",
    "    # --- Compute BOTH metrics ---\n",
    "    f2, pr_auc = cv_eval(preds_baseline)\n",
    "    scores_f2.append(f2)\n",
    "    scores_pr.append(pr_auc)\n",
    "\n",
    "    print(f\"F2 Score (Fold {i+1}): {f2:.4f}\")\n",
    "    print(f\"PR-AUC (Fold {i+1}): {pr_auc:.4f}\")\n",
    "\n",
    "    # --- Plot only if needed ---\n",
    "    if plot_pr:   \n",
    "        plt.figure(figsize=(7, 5))\n",
    "        plt.bar([\"F2 Score\", \"PR-AUC\"], [f2, pr_auc], color=[\"lightcoral\", \"skyblue\"])\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.title(f\"Fold {i+1} — Baseline Metrics\")\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid(True, axis='y')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "152cfd4f-236c-4437-9e8b-78cb2d2cb210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "#mean of F2 score and PR-AUC\n",
    "f2_mean = np.mean(scores_f2)\n",
    "f2_std  = np.std(scores_f2)\n",
    "\n",
    "pr_mean = np.mean(scores_pr)\n",
    "pr_std  = np.std(scores_pr)\n",
    "\n",
    "print(f\"F2 Baseline: {f2_mean:.4f} ± {f2_std:.4f}\")\n",
    "print(f\"PR-AUC Baseline: {pr_mean:.4f} ± {pr_std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2893dd8c-3c4d-4e94-82a3-784d3b29383c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# Add prediction column\n",
    "preds_baseline = val_df.withColumn(\"prediction\", F.lit(float(majority_class)))\n",
    "\n",
    "# For compatibility with binary metrics, also create a probability vector\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Create probability vector [P(0), P(1)]\n",
    "def prob_vector(label):\n",
    "    return Vectors.dense([1.0, 0.0]) if label == 0 else Vectors.dense([0.0, 1.0])\n",
    "\n",
    "prob_udf = udf(prob_vector, VectorUDT())\n",
    "preds_baseline = preds_baseline.withColumn(\"probability\", prob_udf(F.lit(majority_class)))\n",
    "\n",
    "pr_auc = cv_eval(preds_baseline)[1]\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f82de6d9-e31d-4c34-8f0e-c47290bd9ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "def run_model_cv(folds_input_path, N_SPLITS, pipeline, model, plot_pr=False):\n",
    "    metrics_f2 = []\n",
    "    metrics_pr = []\n",
    "\n",
    "    for i in range(N_SPLITS):\n",
    "        print(f\"\\n===== Fold {i+1} =====\")\n",
    "\n",
    "        train_df = spark.read.parquet(f\"{folds_input_path}/fold_{i+1}_train\")\n",
    "        val_df = spark.read.parquet(f\"{folds_input_path}/fold_{i+1}_val\")\n",
    "\n",
    "        # Fit pipeline + model\n",
    "        pipeline_model = pipeline.fit(train_df)\n",
    "        preds = pipeline_model.transform(val_df)\n",
    "\n",
    "        # Evaluate\n",
    "        f2, pr_auc = cv_eval(preds)\n",
    "        metrics_f2.append(f2)\n",
    "        metrics_pr.append(pr_auc)\n",
    "\n",
    "        print(f\"Model F2 (Fold {i+1}): {f2:.4f}\")\n",
    "        print(f\"Model PR-AUC (Fold {i+1}): {pr_auc:.4f}\")\n",
    "\n",
    "        if plot_pr:\n",
    "            plt.figure(figsize=(7,5))\n",
    "            plt.bar([\"F2\", \"PR-AUC\"], [f2, pr_auc])\n",
    "            plt.ylim(0, 1)\n",
    "            plt.title(f\"Fold {i+1} — Model Scores\")\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.show()\n",
    "\n",
    "    return {\n",
    "        \"f2_scores\": metrics_f2,\n",
    "        \"pr_auc_scores\": metrics_pr,\n",
    "        \"f2_mean\": float(np.mean(metrics_f2)),\n",
    "        \"f2_std\": float(np.std(metrics_f2)),\n",
    "        \"pr_auc_mean\": float(np.mean(metrics_pr)),\n",
    "        \"pr_auc_std\": float(np.std(metrics_pr)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3d92752a-85da-48b4-b0af-d831376f3159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "# Your preprocessing + model pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders + [model])\n",
    "\n",
    "# ---- RUN FOR 3 MONTHS ----\n",
    "baseline_3m, model_3m = run_full_pipeline(\n",
    "    time_window=3,\n",
    "    model=model,\n",
    "    pipeline=pipeline,\n",
    "    dbfs_base=dbfs_base,\n",
    "    plot_pr=True\n",
    ")\n",
    "\n",
    "# # ---- RUN FOR 1 YEAR ----\n",
    "# baseline_1y, model_1y = run_full_pipeline(\n",
    "#     time_window=12,\n",
    "#     model=model,\n",
    "#     pipeline=pipeline,\n",
    "#     dbfs_base=dbfs_base,\n",
    "#     plot_pr=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f0a1af-cfda-456a-aa6b-b79c428d6fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "#run full pipeline\n",
    "def run_full_pipeline(time_window, model, pipeline, dbfs_base, plot_pr=True):\n",
    "    time_length = data_set(time_window)\n",
    "    splits_path = f\"{dbfs_base}/splits{time_length}\"\n",
    "    folds_input_path = f\"{splits_path}/scaled_smote_folds\"\n",
    "    N_SPLITS = 4\n",
    "\n",
    "    print(f\"\\n==== RUNNING BASELINE for {time_window}-month window ====\")\n",
    "    baseline_results = evaluate_baseline_on_folds(\n",
    "        folds_input_path, \n",
    "        N_SPLITS=N_SPLITS,\n",
    "        plot_pr=plot_pr\n",
    "    )\n",
    "\n",
    "    print(\"\\n==== RUNNING MODEL CV ====\")\n",
    "    model_results = run_model_cv(\n",
    "        folds_input_path,\n",
    "        N_SPLITS=N_SPLITS,\n",
    "        pipeline=pipeline,\n",
    "        model=model,\n",
    "        plot_pr=plot_pr\n",
    "    )\n",
    "\n",
    "    # Save results to DBFS as Parquet\n",
    "    import pandas as pd\n",
    "    results_df = pd.DataFrame([{\n",
    "        \"time_window\": time_window,\n",
    "        \"baseline\": baseline_results,\n",
    "        \"model\": model_results\n",
    "    }])\n",
    "    spark_df = spark.createDataFrame(results_df)\n",
    "    results_parquet_path = f\"/student-groups/Group_02_01/splits{time_length}/results_{time_length}.parquet\"\n",
    "    spark_df.write.mode(\"overwrite\").parquet(results_parquet_path)\n",
    "\n",
    "    print(f\"\\nResults saved to: {results_parquet_path}\")\n",
    "\n",
    "    return baseline_results, model_results"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "M_03_RandomForest_w_MLFlow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}